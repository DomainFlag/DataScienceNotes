{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding for wine reviews and sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "    \n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import helper as hlp\n",
    "import nltk\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor as RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor as DecisionTreeRegressor\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "raw = pd.read_csv(\"./data/wines/wine_reviews.csv\", low_memory = False);\n",
    "\n",
    "# dropping unnecessary column\n",
    "raw = raw.drop(columns = [\"Unnamed: 0\"], inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(raw, column, punctuation):\n",
    "    ''' utility function for text transformation for machine to interpret '''\n",
    "    \n",
    "    # make dataframe copy\n",
    "    data_copy = raw.copy()\n",
    "    \n",
    "    # transform each row to lowercase\n",
    "    data_copy[column] = data_copy[column].str.lower()\n",
    "    \n",
    "    # filter out punctuation\n",
    "    data_copy[column] = data_copy[column].str.replace('[^\\w\\s]', '')\n",
    "        \n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform each review to lowercase and remove punctuation\n",
    "raw_data = transform_text(raw, \"description\", punctuation)\n",
    "\n",
    "# transform non-numerical data to categorical\n",
    "hlp.trans_categorical(raw_data, labels = [\"description\"])\n",
    "\n",
    "# transform/normalize numerical data\n",
    "features, targets = hlp.trans_numerical(raw_data, \"points\", suffle_data_frame = True)\n",
    "\n",
    "# training and validation data\n",
    "training_set, validation_set = hlp.split_data(features, targets, threshold = 1 / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(training_set[0][\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tdm = tfidf.fit_transform(training_set[0][\"description\"])\n",
    "\n",
    "valid_tdm = tfidf.transform(validation_set[0][\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x43343 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 53 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.transform(validation_set[0][\"description\"].iloc[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(train_tdm, training_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5419663372456802\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(validation_set[1], lr.predict(valid_tdm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_verbose(description, model, transformer, expected_score = None):\n",
    "    \n",
    "    # generate tf-idf-weighted document-term matrix\n",
    "    description_tdm = transformer.transform([description])\n",
    "\n",
    "    # predict score given description\n",
    "    score = model.predict(description_tdm)[0]\n",
    "\n",
    "    print(f\"{description:100.100}...\\\n",
    "          \\n\\t output {score}, expected {expected_score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(data, model, transformer, count = 5):\n",
    "    \n",
    "    for index in range(count):\n",
    "        \n",
    "        # current description\n",
    "        description = data[0][\"description\"][index]\n",
    "        \n",
    "        # expected output\n",
    "        score = validation_set[1][index]\n",
    "        \n",
    "        validation_verbose(description, model, transformer, expected_score = score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet ros√© from the vinho verde region this has some fresh raspberry and a light prickle to make it ...          \n",
      "\t output 85, expected 82\n",
      "\n",
      "aromas of ripe darkskinned fruit leather clove rose and a whiff of menthol set the tone the firmly s...          \n",
      "\t output 90, expected 91\n",
      "\n",
      "this cabernet sauvignonbased wine is a blend of all the bordeaux varieties including carmenere its s...          \n",
      "\t output 86, expected 88\n",
      "\n",
      "aromas of ripe blackberry brown spice and fragrant blue flower waft from the glass a blend of 50 neg...          \n",
      "\t output 88, expected 89\n",
      "\n",
      "cutting streaks of lime and lemon intensify the earthy wet riverrock notes in this complex dry riesl...          \n",
      "\t output 92, expected 94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# some input from validation set\n",
    "validation(validation_set, lr, tfidf, count = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty bad, can't handle the taste, extremely sour, how can someone make such wine?                 ...          \n",
      "\t output 82, expected None\n",
      "\n",
      "Amazing, fine vintage, delicious, rich texture that sobbing for more takes, just pure quality.      ...          \n",
      "\t output 93, expected None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# some custom input\n",
    "\n",
    "# average score\n",
    "validation_verbose(\"Pretty bad, can't handle the taste, extremely sour, how can someone make such wine?\", lr, tfidf)\n",
    "\n",
    "# good score\n",
    "validation_verbose(\"Amazing, fine vintage, delicious, rich texture that sobbing for more takes, just pure quality.\", lr, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Algorithms:\n",
    "    \n",
    "1. Embedding Layer\n",
    "2. Word2Vec\n",
    "    1. CBOW\n",
    "    2. C. Skip-Gram\n",
    "3. GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformer():\n",
    "    ''' utility for data transformations '''\n",
    "    \n",
    "    def __init__(self, raw, target):\n",
    "        \n",
    "        self.raw = raw\n",
    "        self.target = target\n",
    "        \n",
    "        self.raw_copy = raw.copy()\n",
    "    \n",
    "    def tokenize(self):\n",
    "\n",
    "        # tokenize, split a sentence by space\n",
    "        self.raw_copy[self.target] = self.raw[self.target].str.split()\n",
    "        \n",
    "        # find maximum size of sequence of tokens\n",
    "        self.sequence_max = max([ len(review) for review in self.raw_copy[self.target] ])\n",
    "        \n",
    "    def create_vocabulary(self):\n",
    "        ''' create the vocabulary base from the known data '''\n",
    "        \n",
    "        # count token occurrences\n",
    "        self.tokens = Counter([ token for review in self.raw_copy[self.target] for token in review ])\n",
    "        \n",
    "        # vocabulary_size\n",
    "        self.vocabulary_size = len(self.tokens)\n",
    "        \n",
    "        # word to integer mapping\n",
    "        self.word_to_int = { key : index for index, key in enumerate(self.tokens) }\n",
    "        \n",
    "        # integer to word mapping\n",
    "        self.int_to_word = { index : word for word, index in self.word_to_int.items() }\n",
    "        \n",
    "    def apply_to_numerical(self):\n",
    "        ''' apply numerical transformation '''\n",
    "        \n",
    "        # transform from textual to numerical representation\n",
    "        self.raw_copy[self.target] = [ [ self.word_to_int[token] for token in review ] for review in self.raw_copy[self.target] ]\n",
    "        \n",
    "    def apply_to_textual(self):\n",
    "        ''' apply textual transformation '''\n",
    "        \n",
    "        # transform from textual to numerical representation\n",
    "        self.raw_copy[self.target] = [ [ self.word_to_int[token] for token in review ] for review in self.raw_copy[self.target] ]\n",
    "        \n",
    "    def apply_padding(self):\n",
    "        ''' apply padding to numerical content '''\n",
    "        \n",
    "        # assert numerical representation of input data\n",
    "        assert(all(isinstance(token, int) for review in self.raw_copy[self.target] for token in review))\n",
    "         \n",
    "        # transform by padding\n",
    "        self.raw_copy[self.target] = [ review + [-1] * (len(review) - self.sequence_max) for review in self.raw_copy[self.target] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transformer to our data\n",
    "transformer = TextTransformer(raw_data, \"description\")\n",
    "\n",
    "transformer.tokenize()\n",
    "transformer.create_vocabulary()\n",
    "transformer.apply_to_numerical()\n",
    "transformer.apply_padding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'helper' has no attribute 'split_target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-5d396f7a8c6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# training and validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_copy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"description\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# training and validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtraining_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'helper' has no attribute 'split_target'"
     ]
    }
   ],
   "source": [
    "# training and validation data\n",
    "features, targets = hlp.split_target(transformer.raw_copy, \"description\")\n",
    "\n",
    "# training and validation data\n",
    "training_dataset, validation_dataset = hlp.split_target(features, targets, threshold = 1 / 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_size, num_layers, output_size, bidirectional = False):\n",
    "        \n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # sparse(embedding) layer\n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        \n",
    "        # recurrent neural network layer(lstm)\n",
    "        self.rnn = nn.LSTM(self.embedding_dim, self.hidden_size, self.num_layers, bidirectional = self.bidirectional)\n",
    "        \n",
    "        # dropout layyer\n",
    "        self.drp = nn.Dropout(0.15)\n",
    "        \n",
    "        # linear fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # embed words into dense representation\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # recurrent neural network; pass forward\n",
    "        x = self.rnn(x)\n",
    "        \n",
    "        # fully connected layer; pass forward while casually dropping cells\n",
    "        x = self.drp(self.fc(x))\n",
    "        \n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model model(1024, 600, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-2b730490c568>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-2b730490c568>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    F.\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
