{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding for wine reviews and sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "    \n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import helper as hlp\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor as RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor as DecisionTreeRegressor\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "raw = pd.read_csv(\"./data/wines/wine_reviews.csv\", low_memory = False);\n",
    "\n",
    "# dropping unnecessary column\n",
    "raw = raw.drop(columns = [\"Unnamed: 0\"], inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(data, column, punctuation):\n",
    "    ''' utility function for text transformation for machine to interpret '''\n",
    "    \n",
    "    # make dataframe copy\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # transform each row to lowercase\n",
    "    data_copy[column] = data_copy[column].str.lower()\n",
    "    \n",
    "    # filter out punctuation\n",
    "    data_copy[column] = data_copy[column].str.replace('[^\\w\\s]', '')\n",
    "        \n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform each review to lowercase and remove punctuation\n",
    "raw_data = transform_text(raw, \"description\", punctuation)\n",
    "\n",
    "# transform non-numerical data to categorical\n",
    "hlp.trans_categorical(raw_data, labels = [\"description\"])\n",
    "\n",
    "# transform/normalize numeric data\n",
    "raw_numeric_data = hlp.transform_to_numeric(raw_data, suffle_data_frame = True)\n",
    "\n",
    "# split into features and targets\n",
    "features, targets = hlp.split_target(raw_numeric_data, \"points\")\n",
    "\n",
    "# training and validation data\n",
    "training_set, validation_set = hlp.split_data(features, targets, threshold = 1 / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(training_set[0][\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tdm = tfidf.fit_transform(training_set[0][\"description\"])\n",
    "\n",
    "valid_tdm = tfidf.transform(validation_set[0][\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.transform(validation_set[0][\"description\"].iloc[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(train_tdm, training_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(validation_set[1], lr.predict(valid_tdm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_verbose(description, model, transformer, expected_score = None):\n",
    "    \n",
    "    # generate tf-idf-weighted document-term matrix\n",
    "    description_tdm = transformer.transform([description])\n",
    "\n",
    "    # predict score given description\n",
    "    score = model.predict(description_tdm)[0]\n",
    "\n",
    "    print(f\"{description:100.100}...\\\n",
    "          \\n\\t output {score}, expected {expected_score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(data, model, transformer, count = 5):\n",
    "    \n",
    "    for index in range(count):\n",
    "        \n",
    "        # current description\n",
    "        description = data[0][\"description\"][index]\n",
    "        \n",
    "        # expected output\n",
    "        score = validation_set[1][index]\n",
    "        \n",
    "        validation_verbose(description, model, transformer, expected_score = score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some input from validation set\n",
    "validation(validation_set, lr, tfidf, count = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some custom input\n",
    "\n",
    "# average score\n",
    "validation_verbose(\"Pretty bad, can't handle the taste, extremely sour, how can someone make such wine?\", lr, tfidf)\n",
    "\n",
    "# good score\n",
    "validation_verbose(\"Amazing, fine vintage, delicious, rich texture that sobbing for more takes, just pure quality.\", lr, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Algorithms:\n",
    "    \n",
    "1. Embedding Layer\n",
    "2. Word2Vec\n",
    "    1. CBOW\n",
    "    2. C. Skip-Gram\n",
    "3. GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization():\n",
    "    ''' helper class for data normalization and scaling '''\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \n",
    "        self.std = None\n",
    "        self.mean = None\n",
    "        \n",
    "        self.data = data\n",
    "    \n",
    "    def normalize(self, data):\n",
    "        \n",
    "        # compute disparity\n",
    "        self.std = np.std(data)\n",
    "\n",
    "        # compute mean\n",
    "        self.mean = np.mean(data)\n",
    "        \n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def scale(self, data):\n",
    "        \n",
    "        assert(self.std is not None and self.mean is not None)\n",
    "        \n",
    "        return data * self.std + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate instance\n",
    "normalization = Normalization()\n",
    "\n",
    "# targets; scores for each product and features\n",
    "features, targets = raw_data[\"description\"], np.array(raw_data[\"points\"])\n",
    "\n",
    "# normalize scores\n",
    "targets = normalization.normalize(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chainer(ABC):\n",
    "    ''' chainer class for chaining text transformations '''\n",
    "    \n",
    "    @abstractmethod\n",
    "    def process(self, data, chain = None):\n",
    "        ''' chain method for data preprocessing '''\n",
    "        pass\n",
    "    \n",
    "class Tokenize(Chainer):\n",
    "    \n",
    "    def process(self, data, chain):\n",
    "        ''' chain method for data preprocessing '''\n",
    "        \n",
    "        # tokenize, split a sentence by space\n",
    "        chain.data = data.str.split()\n",
    "        \n",
    "        # find maximum size of sequence of tokens\n",
    "        chain.sequence_max = max([ len(sequence) for sequence in chain.data ])\n",
    "        \n",
    "        return (chain.data, chain)\n",
    "    \n",
    "class Vocabulary(Chainer):\n",
    "    \n",
    "    def process(self, data, chain):\n",
    "        ''' create the known vocabulary basis '''\n",
    "        \n",
    "        # count token occurrences\n",
    "        chain.tokens = Counter([ token for sequence in data for token in sequence ])\n",
    "        \n",
    "        # vocabulary_size\n",
    "        chain.vocabulary_size = len(chain.tokens) + 1\n",
    "        \n",
    "        # word to integer mapping, 0 is reserved for padding\n",
    "        chain.word_to_int = { key : (index + 1) for index, key in enumerate(chain.tokens) }\n",
    "        \n",
    "        # integer to word mapping\n",
    "        chain.int_to_word = { index : word for word, index in chain.word_to_int.items() }\n",
    "        \n",
    "        return (data, chain)\n",
    "    \n",
    "class NumericToToken(Chainer):\n",
    "    \n",
    "    def process(self, data, chain):\n",
    "        ''' apply textual transformation '''\n",
    "        \n",
    "        assert(hasattr(chain, 'int_to_word'))\n",
    "        \n",
    "        # transform from textual to numerical representation\n",
    "        chain.data = [ [ chain.int_to_word[token] for token in sequence ] for sequence in data ]\n",
    "        \n",
    "        return (chain.data, chain)\n",
    "\n",
    "class TokenToNumeric(Chainer):\n",
    "    \n",
    "    def process(self, data, chain):\n",
    "        ''' apply numerical transformation '''\n",
    "\n",
    "        assert(hasattr(chain, 'word_to_int'))\n",
    "\n",
    "        # transform from textual to numerical representation\n",
    "        chain.data = [ [ chain.word_to_int[token] for token in sequence ] for sequence in data ]\n",
    "\n",
    "        return (chain.data, chain)\n",
    "           \n",
    "class Filler(Chainer):\n",
    "    \n",
    "    def process(self, data, chain):\n",
    "        ''' apply padding to numerical content '''\n",
    "\n",
    "        # assert numerical representation of input data\n",
    "        assert(all(isinstance(token, int) for sequence in data for token in sequence))\n",
    "        assert(hasattr(chain, 'sequence_max'))\n",
    "\n",
    "        # transform by padding\n",
    "        chain.data = [ sequence + [0] * (chain.sequence_max - len(sequence)) for sequence in data ]\n",
    "        \n",
    "        return (np.array(chain.data), chain)\n",
    "\n",
    "class Composer(Chainer):\n",
    "    \n",
    "    def __init__(self, transforms):\n",
    "        \n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def process(self, data, chain = None):\n",
    "        \n",
    "        # initialize chainer data\n",
    "        self.data = data\n",
    "        \n",
    "        # apply transformations in series\n",
    "        for transform in self.transforms:\n",
    "            \n",
    "            # check if it's chainer transformer\n",
    "            if(isinstance(transform, Chainer)):\n",
    "                \n",
    "                # update existing data, pass only current class reference\n",
    "                self.data, _ = transform.process(self.data, self)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # list item is not an instance of Chainer transformer\n",
    "                raise Exception(\"Illegal parameter, provide contiguous set of Chainer(s)\")\n",
    "                \n",
    "        return (self.data, self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transformations to our data\n",
    "features, chainer = Composer([\n",
    "    Tokenize(),\n",
    "    Vocabulary(),\n",
    "    TokenToNumeric(),\n",
    "    Filler()\n",
    "]).process(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation dataset\n",
    "train_dataset, valid_dataset = hlp.split_data(features, targets, threshold = 1 / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tensor datasets\n",
    "train_tensor_dataset = TensorDataset(torch.from_numpy(train_dataset[0]).long(), torch.from_numpy(train_dataset[1]))\n",
    "valid_tensor_dataset = TensorDataset(torch.from_numpy(valid_dataset[0]).long(), torch.from_numpy(valid_dataset[1]))\n",
    "\n",
    "# create data loaders\n",
    "train_loader = DataLoader(train_tensor_dataset, batch_size = 16, shuffle = True)\n",
    "valid_loader = DataLoader(valid_tensor_dataset, batch_size = 16, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_size, num_layers, output_size, bidirectional = False):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # sparse(embedding) layer\n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        \n",
    "        # recurrent neural network layer(lstm)\n",
    "        self.rnn = nn.LSTM(self.embedding_dim, self.hidden_size, self.num_layers, bidirectional = self.bidirectional)\n",
    "        \n",
    "        # fully connected layer(linear) + dropout\n",
    "        self.sec = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.output_size),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # embed words into dense representation\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # recurrent neural network; pass forward\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # fully connected layer; pass forward while casually dropping cells\n",
    "        x = self.sec(x)\n",
    "        \n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model\n",
    "model = Model(chainer.vocabulary_size, 500, 480, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context():\n",
    "    \n",
    "    def __init__(self, model, learning_rate):\n",
    "        \n",
    "        # device to be used\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # model to be trained\n",
    "        self.model = model.to(self.device)\n",
    "        \n",
    "        # learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # loss function (mean squared error loss)\n",
    "        self.criterion = nn.MSELoss(reduction = 'mean')\n",
    "\n",
    "        # optimizer with momentum\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "    \n",
    "    def create_scheduler(self):\n",
    "        \n",
    "        # custom scheduler\n",
    "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones = [4, 9], gamma = 0.72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context container for model characteristics\n",
    "context = Context(model, learning_rate = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# singleton batch sample\n",
    "feature_sample, target_sample = next(iter(train_loader))\n",
    "\n",
    "# move to corresponding available\n",
    "feature_sample, target_sample = feature_sample.to(context.device), target_sample.to(context.device)\n",
    "\n",
    "# forward pass\n",
    "output = model.forward(feature_sample)\n",
    "\n",
    "# compute loss\n",
    "loss = context.criterion(output, target_sample.float())\n",
    "\n",
    "print(f\"Loss is {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
