{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PuGA_XMfbtXs"
   },
   "source": [
    "### Word embedding for wine reviews and sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# development method either \"local\" or \"remote\"\n",
    "development = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4yLTh4h3e-jC"
   },
   "outputs": [],
   "source": [
    "# root path if local\n",
    "root_path = \".\"\n",
    "\n",
    "if development == \"remote\":\n",
    "    \n",
    "    from google.colab import drive\n",
    "\n",
    "    # mounting google drive to system\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # root path if remote\n",
    "    root_path = '/content/drive/My Drive/word_embedding'\n",
    "\n",
    "# module path\n",
    "module_path = root_path + \"/..\";\n",
    "\n",
    "# model path\n",
    "model_path = root_path + \"/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z1vWpdAwe9LE"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "    \n",
    "sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "panMB5qGbtXx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import helper as hlp\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor as RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor as DecisionTreeRegressor\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 707,
     "status": "ok",
     "timestamp": 1558644537478,
     "user": {
      "displayName": "Cristian Chivriga",
      "photoUrl": "",
      "userId": "12583023746426163942"
     },
     "user_tz": -120
    },
    "id": "l9USTp29btXz",
    "outputId": "9ba037b8-68ef-428c-b433-4b8b3eadd3bc"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szAjIBo1btX2"
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "raw = pd.read_csv(root_path + \"/data/wines/wine_reviews.csv\", low_memory = False);\n",
    "\n",
    "# dropping unnecessary column\n",
    "raw = raw.drop(columns = [\"Unnamed: 0\"], inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0SznicBbbtX4"
   },
   "outputs": [],
   "source": [
    "def transform_text(data, column, punctuation):\n",
    "    ''' utility function for text transformation for the machine to interpret '''\n",
    "    \n",
    "    # make dataframe copy\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # transform each row to lowercase\n",
    "    data_copy[column] = data_copy[column].str.lower()\n",
    "    \n",
    "    # filter out punctuation\n",
    "    data_copy[column] = data_copy[column].str.replace('[^\\w\\s]', '')\n",
    "        \n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vGlG6qVO6nIv"
   },
   "outputs": [],
   "source": [
    "# transform each review to lowercase and remove punctuation\n",
    "raw_data = transform_text(raw, \"description\", punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1567,
     "status": "error",
     "timestamp": 1558629899853,
     "user": {
      "displayName": "Cristian Chivriga",
      "photoUrl": "",
      "userId": "12583023746426163942"
     },
     "user_tz": -120
    },
    "id": "-oM-fQ3cbtX6",
    "outputId": "ee2dd3c2-800b-4849-d744-b8466769876c"
   },
   "outputs": [],
   "source": [
    "# transform non-numerical data to categorical\n",
    "hlp.trans_categorical(raw_data, labels = [\"description\"])\n",
    "\n",
    "# transform/normalize numeric data\n",
    "raw_numeric_data = hlp.transform_to_numeric(raw_data, suffle_data_frame = True)\n",
    "\n",
    "# training and validation set\n",
    "train_set, valid_set = hlp.split_data(raw_numeric_data, threshold = 1 / 8)\n",
    "\n",
    "# targets; scores for each product and features\n",
    "train_dataset, valid_dataset = hlp.split_features(train_set, valid_set, columns = [\"description\", \"points\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6PGZLnQ6btX8"
   },
   "outputs": [],
   "source": [
    "transformer = TfidfVectorizer(stop_words = stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQDlLcGMbtYA"
   },
   "outputs": [],
   "source": [
    "# learning vocabulary and get the term-document matrix\n",
    "train_term_document = transformer.fit_transform(train_dataset[1])\n",
    "\n",
    "# based on learned vocabulary transform validation set\n",
    "valid_term_document = transformer.transform(valid_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_term_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SJ3YLVjsbtYI"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "18-Ge_YEbtYI"
   },
   "outputs": [],
   "source": [
    "# create our model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# fit data\n",
    "model.fit(train_term_document, train_dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mld27AYlbtYK"
   },
   "outputs": [],
   "source": [
    "print(r2_score(valid_dataset[2], model.predict(valid_term_document)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6CWe4SicbtYM"
   },
   "outputs": [],
   "source": [
    "def validation_verbose(description, model, transformer, expected_score = None):\n",
    "    \n",
    "    # generate tf-idf-weighted document-term matrix\n",
    "    description_tdm = transformer.transform([description])\n",
    "\n",
    "    # predict score given description\n",
    "    score = model.predict(description_tdm)[0]\n",
    "\n",
    "    print(f\"{description:100.100}...\\\n",
    "          \\n\\t output {score}, expected {expected_score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MCZMb0d9btYO"
   },
   "outputs": [],
   "source": [
    "def validation(data, model, transformer, count = 5):\n",
    "    \n",
    "    for index in range(count):\n",
    "        \n",
    "        # current description\n",
    "        description = data[0][\"description\"][index]\n",
    "        \n",
    "        # expected output\n",
    "        score = validation_set[1][index]\n",
    "        \n",
    "        validation_verbose(description, model, transformer, expected_score = score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GD7rw_TybtYQ"
   },
   "outputs": [],
   "source": [
    "# some input from validation set\n",
    "validation(validation_set, lr, tfidf, count = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61wyLN29btYT"
   },
   "outputs": [],
   "source": [
    "# some custom input\n",
    "\n",
    "# average score\n",
    "validation_verbose(\"Pretty bad, can't handle the taste, extremely sour, how can someone make such wine?\", lr, tfidf)\n",
    "\n",
    "# good score\n",
    "validation_verbose(\"Amazing, fine vintage, delicious, rich texture that sobbing for more takes, just pure quality.\", lr, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrB9aP5-btYW"
   },
   "source": [
    "### Word Embedding Algorithms:\n",
    "    \n",
    "1. Embedding Layer\n",
    "2. Word2Vec\n",
    "    1. CBOW\n",
    "    2. C. Skip-Gram\n",
    "3. GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5k5s2RjdbtYX"
   },
   "outputs": [],
   "source": [
    "# batch size\n",
    "batch_size = 16\n",
    "\n",
    "# training and validation set\n",
    "train_set, valid_set = hlp.split_data(raw_data, threshold = 1 / 8, batch_trim = batch_size)\n",
    "\n",
    "# targets; scores for each product and features\n",
    "train_dataset, valid_dataset = hlp.split_features(train_set, valid_set, columns = [\"description\", \"points\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PYZcyaI4btYZ"
   },
   "outputs": [],
   "source": [
    "class Chainer(ABC):\n",
    "    ''' chainer class for chaining text transformations '''\n",
    "    \n",
    "    @abstractmethod\n",
    "    def process(self, data, chain = None):\n",
    "        ''' chain method for data preprocessing '''\n",
    "        \n",
    "        pass\n",
    "    \n",
    "class Tokenize(Chainer):\n",
    "    \n",
    "    def process(self, data, chain):\n",
    "        ''' chain method for data preprocessing '''\n",
    "        \n",
    "        # tokenize, split a sentence by space\n",
    "        chain.data = data.str.split()\n",
    "        \n",
    "        # find maximum size of sequence of tokens\n",
    "        chain.sequence_max = max([ len(sequence) for sequence in chain.data ])\n",
    "        \n",
    "        return (chain.data, chain)\n",
    "    \n",
    "class Vocabulary(Chainer):\n",
    "    \n",
    "    def process(self, data, chain):\n",
    "        ''' create the known vocabulary basis '''\n",
    "        \n",
    "        # count token occurrences\n",
    "        chain.tokens = Counter([ token for sequence in data for token in sequence ])\n",
    "        \n",
    "        # vocabulary_size\n",
    "        chain.vocabulary_size = len(chain.tokens) + 1\n",
    "        \n",
    "        # word to integer mapping, 0 is reserved for padding\n",
    "        chain.word_to_int = { key : (index + 1) for index, key in enumerate(chain.tokens) }\n",
    "        \n",
    "        # integer to word mapping\n",
    "        chain.int_to_word = { index : word for word, index in chain.word_to_int.items() }\n",
    "        \n",
    "        return (data, chain)\n",
    "    \n",
    "class NumericToToken(Chainer):\n",
    "    \n",
    "    def process(self, data, chain):\n",
    "        ''' apply textual transformation '''\n",
    "        \n",
    "        assert(hasattr(chain, 'int_to_word'))\n",
    "        \n",
    "        # transform from textual to numerical representation\n",
    "        chain.data = [ \n",
    "            [ chain.int_to_word[token] for token in sequence if token in chain.int_to_word ] \n",
    "        for sequence in data ]\n",
    "        \n",
    "        return (chain.data, chain)\n",
    "\n",
    "class TokenToNumeric(Chainer):\n",
    "    \n",
    "    def process(self, data, chain):\n",
    "        ''' apply numerical transformation '''\n",
    "\n",
    "        assert(hasattr(chain, 'word_to_int'))\n",
    "\n",
    "        # transform from textual to numerical representation\n",
    "        chain.data = [ \n",
    "            [ chain.word_to_int[token] if token in chain.word_to_int else 0 for token in sequence ] \n",
    "        for sequence in data ]\n",
    "\n",
    "        return (chain.data, chain)\n",
    "           \n",
    "class Filler(Chainer):\n",
    "    \n",
    "    def process(self, data, chain):\n",
    "        ''' apply padding to numerical content '''\n",
    "\n",
    "        # assert numerical representation of input data\n",
    "        assert(all(isinstance(token, int) for sequence in data for token in sequence))\n",
    "        assert(hasattr(chain, 'sequence_max'))\n",
    "\n",
    "        # get the real size of each sequence\n",
    "        chain.sizes = np.array([ len(sequence) - 1 for sequence in data ])\n",
    "\n",
    "        # transform by padding\n",
    "        chain.data = [ sequence + [0] * (chain.sequence_max - len(sequence)) for sequence in data ]\n",
    "        \n",
    "        return (np.array(chain.data), chain)\n",
    "\n",
    "class Composer(Chainer):\n",
    "    \n",
    "    def __init__(self, transforms):\n",
    "        \n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def process(self, data, chain = None):\n",
    "        \n",
    "        # initialize chainer data\n",
    "        self.data = data\n",
    "        \n",
    "        # available chainer\n",
    "        chain = self if chain is None else chain\n",
    "        \n",
    "        # apply transformations in series\n",
    "        for transform in self.transforms:\n",
    "            \n",
    "            # check if it's chainer transformer\n",
    "            if(isinstance(transform, Chainer)):\n",
    "                \n",
    "                # update existing data, pass only current class reference\n",
    "                self.data, _ = transform.process(self.data, chain)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # list item is not an instance of Chainer transformer\n",
    "                raise Exception(\"Illegal parameter, provide contiguous set of Chainer(s)\")\n",
    "                \n",
    "        return (self.data, self)\n",
    "    \n",
    "    def transform(self, data, chain):\n",
    "        \n",
    "        # apply transformation given learned vocabulary\n",
    "        features, _ = Composer([\n",
    "            Tokenize(),\n",
    "            TokenToNumeric(),\n",
    "            Filler()\n",
    "        ]).process(data, chain = chain)\n",
    "        \n",
    "        return (features, chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Az7vX2KHbtYa"
   },
   "outputs": [],
   "source": [
    "# apply transformations and learn the vocabulary of our train dataset\n",
    "train_dataset[1], train_chainer = Composer([\n",
    "    Tokenize(),\n",
    "    Vocabulary(),\n",
    "    TokenToNumeric(),\n",
    "    Filler()\n",
    "]).process(train_dataset[1])\n",
    "\n",
    "\n",
    "# apply transformations given existing learned vocabulary\n",
    "valid_dataset[1], valid_chainer = chainer.transform(valid_dataset[1], chainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXvPGZSubtYe"
   },
   "outputs": [],
   "source": [
    "# create the tensor datasets\n",
    "train_tensor_dataset = TensorDataset(torch.from_numpy(train_dataset[1]).long(), torch.from_numpy(train_dataset[2].values), torch.from_numpy(chainer.sizes))\n",
    "valid_tensor_dataset = TensorDataset(torch.from_numpy(valid_dataset[1]).long(), torch.from_numpy(valid_dataset[2].values), torch.from_numpy(chainer.sizes))\n",
    "\n",
    "# create data loaders\n",
    "train_loader = DataLoader(train_tensor_dataset, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_tensor_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b48nBt_QbtYf"
   },
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jho_unKcbtYf"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_size, num_layers, output_size, bidirectional = False):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # sparse(embedding) layer\n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        \n",
    "        # recurrent neural network layer(lstm)\n",
    "        self.rnn = nn.LSTM(self.embedding_dim, self.hidden_size, self.num_layers, \n",
    "                               bidirectional = self.bidirectional, batch_first = True)\n",
    "        \n",
    "        # fully connected layer(linear) + dropout\n",
    "        self.sec1 = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, 128),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # fully connected layer(linear) + dropout\n",
    "        self.sec2 = nn.Sequential(\n",
    "            nn.Linear(128, self.output_size),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, indices, verbose = False):\n",
    "\n",
    "        # embed words into dense representation\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # recurrent neural network; pass forward\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # stack rnn output\n",
    "        x = x.contiguous()\n",
    "        \n",
    "        # fully connected layer; pass forward while casually dropping cells\n",
    "        x = self.sec1(x)\n",
    "        \n",
    "        # fully connected layer; pass forward while casually dropping cells\n",
    "        x = self.sec2(x)\n",
    "        \n",
    "        # extract last prediction\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        return x[torch.arange(len(indices)), indices]\n",
    "    \n",
    "    def save_model(self, checkpoint):\n",
    "        ''' save model to dictionary '''\n",
    "        \n",
    "        assert(isinstance(checkpoint, dict))\n",
    "        \n",
    "        checkpoint[\"num_embeddings\"] = self.num_embeddings\n",
    "        checkpoint[\"embedding_dim\"] = self.embedding_dim\n",
    "        checkpoint[\"hidden_size\"] = self.hidden_size\n",
    "        checkpoint[\"num_layers\"] = self.num_layers\n",
    "        checkpoint[\"output_size\"] = self.output_size\n",
    "        checkpoint[\"bidirectional\"] = self.bidirectional\n",
    "        \n",
    "        # save model internal weights\n",
    "        checkpoint[\"state_dict\"] = self.state_dict()\n",
    "        \n",
    "        return checkpoint\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_model(checkpoint):\n",
    "        ''' load model from dictionary '''\n",
    "        \n",
    "        assert(isinstance(checkpoint, dict))\n",
    "        \n",
    "        # create an instance of Model\n",
    "        model = Model(\n",
    "            checkpoint[\"num_embeddings\"],\n",
    "            checkpoint[\"embedding_dim\"],\n",
    "            checkpoint[\"hidden_size\"],\n",
    "            checkpoint[\"num_layers\"],\n",
    "            checkpoint[\"output_size\"],\n",
    "            checkpoint[\"bidirectional\"]\n",
    "        )\n",
    "        \n",
    "        # load weights\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COfB7vE6btYh"
   },
   "source": [
    "### Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OLLYl69GbtYh"
   },
   "outputs": [],
   "source": [
    "class Context():\n",
    "    \n",
    "    def __init__(self, model, learning_rate, verbose = True):\n",
    "        \n",
    "        assert(isinstance(model, Model))\n",
    "        \n",
    "        # device to be used\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        if verbose:\n",
    "            \n",
    "            print(self.device)\n",
    "        \n",
    "        # model to be trained\n",
    "        self.model = model.to(self.device)\n",
    "        \n",
    "        # learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # loss function (mean squared error loss)\n",
    "        self.criterion = nn.MSELoss(reduction = 'mean')\n",
    "\n",
    "        # optimizer with momentum\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "        \n",
    "        # current validation min\n",
    "        self.valid_loss_min = np.inf\n",
    "    \n",
    "    def create_scheduler(self):\n",
    "        \n",
    "        # custom scheduler\n",
    "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones = [4, 9], gamma = 0.72)\n",
    "        \n",
    "    def save_context(self, name):\n",
    "        \n",
    "        # current checkpoint to be saved\n",
    "        checkpoint = {\n",
    "            \"state_optim\" : self.optimizer.state_dict(),\n",
    "            \"learning_rate\" : self.learning_rate,\n",
    "            \"valid_loss_min\" : self.valid_loss_min\n",
    "        }\n",
    "        \n",
    "        # save model params to dict\n",
    "        checkpoint = self.model.save_model(checkpoint)\n",
    "        \n",
    "        # save checkpoint to disk\n",
    "        torch.save(checkpoint, f\"{model_path}/{name}\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_context(name, context = None):\n",
    "        \n",
    "        # load latest checkpoint\n",
    "        checkpoint = torch.load(f\"{model_path}/{name}\")\n",
    "        \n",
    "        # model loading\n",
    "        model = Model.load_model(checkpoint)\n",
    "        \n",
    "        # recreate context\n",
    "        context = Context(model, checkpoint[\"learning_rate\"])\n",
    "        \n",
    "        # update current validation loss\n",
    "        context.valid_loss_min = checkpoint[\"valid_loss_min\"]\n",
    "        \n",
    "        # optimizer loading e.g state momentum\n",
    "        context.optimizer.load_state_dict(checkpoint[\"state_optim\"])\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 750,
     "status": "ok",
     "timestamp": 1558643643445,
     "user": {
      "displayName": "Cristian Chivriga",
      "photoUrl": "",
      "userId": "12583023746426163942"
     },
     "user_tz": -120
    },
    "id": "qWg1wX8xbtYj",
    "outputId": "8baafb8a-7405-447e-d971-1ab16b407dc2"
   },
   "outputs": [],
   "source": [
    "# our model\n",
    "model = Model(chainer.vocabulary_size, 512, 256, 2, 1)\n",
    "\n",
    "# context container for model characteristics\n",
    "context = Context(model, learning_rate = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7U4AD_WbtYm"
   },
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 672,
     "status": "ok",
     "timestamp": 1558643646568,
     "user": {
      "displayName": "Cristian Chivriga",
      "photoUrl": "",
      "userId": "12583023746426163942"
     },
     "user_tz": -120
    },
    "id": "a29U7udYbtYm",
    "outputId": "a6c1d842-ad92-4f4f-fec4-7107232f03ea"
   },
   "outputs": [],
   "source": [
    "# singleton batch sample\n",
    "feature_sample, target_sample, indices = next(iter(train_loader))\n",
    "\n",
    "# move to corresponding available\n",
    "feature_sample, target_sample = feature_sample.to(context.device), target_sample.to(context.device)\n",
    "\n",
    "# forward pass\n",
    "output = model.forward(feature_sample, indices)\n",
    "\n",
    "# compute lossop\n",
    "loss = context.criterion(output, target_sample)\n",
    "\n",
    "print(f\"Loss is {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zn0Cu-cibtYp"
   },
   "outputs": [],
   "source": [
    "def valid_model(context, epoch, train_acc_loss):\n",
    "    ''' model validation '''\n",
    "    \n",
    "    # the model to be validated\n",
    "    model = context.model\n",
    "        \n",
    "    # validation loss\n",
    "    valid_acc_loss = 0\n",
    "    \n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for features, targets, indices in valid_loader:\n",
    "\n",
    "            # forward pass\n",
    "            output = model.forward(features, indices)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = context.criterion(output, targets)\n",
    "            \n",
    "            # update validation acc loss\n",
    "            valid_acc_loss += loss.item() \n",
    "\n",
    "    # average loss \n",
    "    valid_acc_loss = valid_acc_loss / len(valid_loader)\n",
    "\n",
    "    # print training / validation statistics \n",
    "    print('Current Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch + 1, train_acc_loss, valid_acc_loss))\n",
    "\n",
    "    if(valid_acc_loss <= context.valid_loss_min):\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            context.valid_loss_min, valid_acc_loss))\n",
    "\n",
    "        # update current best validation loss\n",
    "        context.valid_loss_min = valid_acc_loss\n",
    "\n",
    "        # save best model\n",
    "        context.save_context('model.pt')\n",
    "        \n",
    "\n",
    "def train_model(context, epochs = 1, show_every_step = len(train_loader)):\n",
    "    ''' model training '''\n",
    "    \n",
    "    # the model to be trained\n",
    "    model = context.model\n",
    "    \n",
    "    # train loss and step counter\n",
    "    train_acc_loss, step_counter = 0, 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # training mode\n",
    "        model.train()\n",
    "        \n",
    "        for index, (features, targets, indices) in enumerate(train_loader):\n",
    "            \n",
    "            # removing accumulated gradients\n",
    "            context.optimizer.zero_grad()\n",
    "            \n",
    "            # move to corresponding available device\n",
    "            features, targets = features.to(context.device), targets.to(context.device)\n",
    "            \n",
    "            # forward pass\n",
    "            output = model.forward(features, indices)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = context.criterion(output, targets)\n",
    "            \n",
    "            # calculate gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # adjusting weights\n",
    "            context.optimizer.step()\n",
    "            \n",
    "            # update loss acc\n",
    "            train_acc_loss += loss.item()\n",
    "            \n",
    "            # update step counter\n",
    "            step_counter += 1\n",
    "            \n",
    "            if(step_counter % show_every_step == 0):\n",
    "                \n",
    "                # average loss \n",
    "                train_acc_loss = train_acc_loss / len(train_loader)\n",
    "\n",
    "                # print training / validation statistics \n",
    "                print('Current Epoch: {} \\t Training Loss: {:.6f}'.format(\n",
    "                    epoch + 1, train_acc_loss))\n",
    "                \n",
    "                # reset train and step accumulator\n",
    "                train_acc_loss, step_counter = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0g14C8AYbtYq"
   },
   "outputs": [],
   "source": [
    "# training model\n",
    "train_model(context, 5, show_every_step = 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lx1s9Au3btYu"
   },
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PjAUWQidbtYv"
   },
   "outputs": [],
   "source": [
    "def pred_model(context):\n",
    "    \n",
    "    # evaluation mode\n",
    "    context.model.eval()\n",
    "    \n",
    "    # valid features targets \n",
    "    features, targets, indices = next(iter(train_loader))\n",
    "    \n",
    "    # move to available device\n",
    "    features, targets = features.to(context.device), targets.to(context.device)\n",
    "    \n",
    "    # making prediction\n",
    "    outputs = context.model.forward(features, indices, verbose = True)\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        \n",
    "        print(f\"Predicted score: {outputs[i]}, actual score {targets[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 678,
     "status": "ok",
     "timestamp": 1558639877080,
     "user": {
      "displayName": "Cristian Chivriga",
      "photoUrl": "",
      "userId": "12583023746426163942"
     },
     "user_tz": -120
    },
    "id": "SApcCDlVbtYx",
    "outputId": "c3b111ac-f593-432d-8ad9-37d6d4068004"
   },
   "outputs": [],
   "source": [
    "# making predictions\n",
    "pred_model(context)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "word_embedding.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
