{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word_embedding.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PuGA_XMfbtXs"},"source":["### Word embedding for wine reviews and sentiment analysis"]},{"cell_type":"code","metadata":{"id":"BOtpSfbqZgLO","colab_type":"code","colab":{}},"source":["# development method either \"local\" or \"remote\"\n","development = \"remote\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4yLTh4h3e-jC","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5c0086d4-45c9-4435-8447-27755fe0c576","executionInfo":{"status":"ok","timestamp":1558811463702,"user_tz":-120,"elapsed":819,"user":{"displayName":"Cristian Chivriga","photoUrl":"","userId":"12583023746426163942"}}},"source":["# root path if local\n","root_path = \".\"\n","\n","if development == \"remote\":\n","    \n","    from google.colab import drive\n","\n","    # mounting google drive to system\n","    drive.mount('/content/drive')\n","    \n","    # root path if remote\n","    root_path = '/content/drive/My Drive/word_embedding'\n","\n","# module path\n","module_path = root_path + \"/..\";\n","\n","# model path\n","model_path = root_path + \"/model\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"z1vWpdAwe9LE","colab":{}},"source":["import sys\n","    \n","sys.path.insert(0, module_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"panMB5qGbtXx","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import helper as hlp\n","import nltk\n","import re\n","\n","from abc import ABC, abstractmethod\n","from string import punctuation\n","\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import r2_score, mean_squared_error\n","\n","from sklearn.ensemble import RandomForestRegressor as RandomForestRegressor\n","from sklearn.tree import DecisionTreeRegressor as DecisionTreeRegressor\n","from nltk.corpus import stopwords\n","from collections import Counter\n","\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","from pandas.api.types import is_categorical_dtype\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1558811470220,"user_tz":-120,"elapsed":745,"user":{"displayName":"Cristian Chivriga","photoUrl":"","userId":"12583023746426163942"}},"id":"l9USTp29btXz","outputId":"c61d53cb-4879-4f1b-a639-d8a84d86bd9b","colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["nltk.download('punkt')\n","nltk.download('stopwords')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"szAjIBo1btX2","colab":{}},"source":["# loading data\n","raw = pd.read_csv(root_path + \"/data/wines/wine_reviews.csv\", low_memory = False);\n","\n","# dropping unnecessary column\n","raw = raw.drop(columns = [\"Unnamed: 0\"], inplace = False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0SznicBbbtX4","colab":{}},"source":["def transform_text(data, column, punctuation):\n","    ''' utility function for text transformation for the machine to interpret '''\n","    \n","    # make dataframe copy\n","    data_copy = data.copy()\n","    \n","    # transform each row to lowercase\n","    data_copy[column] = data_copy[column].str.lower()\n","    \n","    # filter out punctuation\n","    data_copy[column] = data_copy[column].str.replace('[^\\w\\s]', '')\n","        \n","    return data_copy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vGlG6qVO6nIv","colab":{}},"source":["# transform each review to lowercase and remove punctuation\n","raw_data = transform_text(raw, \"description\", punctuation)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-oM-fQ3cbtX6","colab":{}},"source":["# transform non-numerical data to categorical\n","hlp.trans_categorical(raw_data, labels = [\"description\"])\n","\n","# transform/normalize numeric data\n","raw_numeric_data = hlp.transform_to_numeric(raw_data, suffle_data_frame = True)\n","\n","# training and validation set\n","train_set, valid_set = hlp.split_data(raw_numeric_data, threshold = 1 / 8)\n","\n","# targets; scores for each product and features\n","train_dataset, valid_dataset = hlp.split_features(train_set, valid_set, columns = [\"description\", \"points\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6PGZLnQ6btX8","colab":{}},"source":["transformer = TfidfVectorizer(stop_words = stopwords.words('english'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LQDlLcGMbtYA","colab":{}},"source":["# learning vocabulary and get the term-document matrix\n","train_term_document = transformer.fit_transform(train_dataset[1])\n","\n","# based on learned vocabulary transform validation set\n","valid_term_document = transformer.transform(valid_dataset[1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SJ3YLVjsbtYI"},"source":["### Logistic Regression"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"18-Ge_YEbtYI","colab":{}},"source":["# create our model\n","model = LogisticRegression()\n","\n","# fit data\n","model.fit(train_term_document, train_dataset[2])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mld27AYlbtYK","colab":{}},"source":["print(r2_score(valid_dataset[2], model.predict(valid_term_document)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6CWe4SicbtYM","colab":{}},"source":["def validation_verbose(description, model, transformer, expected_score = None):\n","    \n","    # generate tf-idf-weighted document-term matrix\n","    description_tdm = transformer.transform([description])\n","\n","    # predict score given description\n","    score = model.predict(description_tdm)[0]\n","\n","    print(f\"{description:100.100}...\\\n","          \\n\\t output {score}, expected {expected_score}\\n\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MCZMb0d9btYO","colab":{}},"source":["def validation(data, model, transformer, count = 5):\n","    \n","    for index in range(count):\n","        \n","        # current description\n","        description = data[0][\"description\"][index]\n","        \n","        # expected output\n","        score = validation_set[1][index]\n","        \n","        validation_verbose(description, model, transformer, expected_score = score)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GD7rw_TybtYQ","colab":{}},"source":["# some input from validation set\n","validation(validation_set, lr, tfidf, count = 5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"61wyLN29btYT","colab":{}},"source":["# some custom input\n","\n","# average score\n","validation_verbose(\"Pretty bad, can't handle the taste, extremely sour, how can someone make such wine?\", lr, tfidf)\n","\n","# good score\n","validation_verbose(\"Amazing, fine vintage, delicious, rich texture that sobbing for more takes, just pure quality.\", lr, tfidf)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YrB9aP5-btYW"},"source":["### Word Embedding Algorithms:\n","    \n","1. Embedding Layer\n","2. Word2Vec\n","    1. CBOW\n","    2. C. Skip-Gram\n","3. GloVe"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5k5s2RjdbtYX","colab":{}},"source":["# batch size\n","batch_size = 16\n","\n","# training and validation set\n","train_set, valid_set = hlp.split_data(raw_data, threshold = 1 / 8, batch_trim = batch_size)\n","\n","# targets; scores for each product and features\n","train_dataset, valid_dataset = hlp.split_features(train_set, valid_set, columns = [\"description\", \"points\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PYZcyaI4btYZ","colab":{}},"source":["class Chainer(ABC):\n","    ''' chainer class for chaining text transformations '''\n","    \n","    @abstractmethod\n","    def process(self, data, chronicle):\n","        ''' chain method for data preprocessing '''\n","        \n","        pass\n","    \n","    def apply(self, data, chronicle):\n","        ''' default action '''\n","    \n","        return self.process(data, chronicle)\n","\n","    \n","class Chronicle():\n","    \n","    def __init__(self, sequence_max = None, word_to_int = None, int_to_word = None, vocabulary_size = None,\\\n","                mean = None, std = None):\n","    \n","        self.sequence_max = sequence_max\n","        self.word_to_int = word_to_int\n","        self.int_to_word = int_to_word\n","        self.vocabulary_size = vocabulary_size\n","        self.mean = mean\n","        self.std = std\n","        \n","    def copy(self):\n","        \n","        # create an independent chronicle\n","        return Chronicle(sequence_max = self.sequence_max, word_to_int = self.word_to_int, int_to_word = self.int_to_word, \n","                         vocabulary_size = self.vocabulary_size, mean = self.mean, std = self.std)\n","    \n","    \n","class Tokenize(Chainer):\n","    \n","    def process(self, data, chronicle):\n","        ''' chain method for data preprocessing '''\n","        \n","        # tokenize, split a sentence by space\n","        data = data.str.split()\n","        \n","        # find maximum size of sequence of tokens\n","        chronicle.sequence_max = max([ len(sequence) for sequence in data ])\n","        \n","        return (data, chronicle)\n","\n","    \n","class Vocabulary(Chainer):\n","    \n","    def process(self, data, chronicle):\n","        ''' create the known vocabulary basis '''\n","        \n","        # count token occurrences\n","        chronicle.tokens = Counter([ token for sequence in data for token in sequence ])\n","        \n","        # vocabulary_size\n","        chronicle.vocabulary_size = len(chronicle.tokens) + 1\n","        \n","        # word to integer mapping, 0 is reserved for padding\n","        chronicle.word_to_int = { key : (index + 1) for index, key in enumerate(chronicle.tokens) }\n","        \n","        # integer to word mapping\n","        chronicle.int_to_word = { index : word for word, index in chronicle.word_to_int.items() }\n","        \n","        return (data, chronicle)\n","    \n","    def apply(self, data, chronicle):\n","        ''' override apply transformation method '''\n","    \n","        # nothing to do\n","        return (data, chronicle)\n","\n","    \n","class NumericToToken(Chainer):\n","    \n","    def process(self, data, chronicle):\n","        ''' apply textual transformation '''\n","        \n","        assert(hasattr(chronicle, 'int_to_word'))\n","        \n","        # transform from textual to numerical representation\n","        data = [ \n","            [ chronicle.int_to_word[token] for token in sequence if token in chronicle.int_to_word ] \n","        for sequence in data ]\n","        \n","        return (data, chronicle)\n","    \n","    \n","class TokenToNumeric(Chainer):\n","    \n","    def process(self, data, chronicle):\n","        ''' apply numerical transformation '''\n","\n","        assert(hasattr(chronicle, 'word_to_int'))\n","\n","        # transform from textual to numerical representation\n","        data = [ \n","            [ chronicle.word_to_int[token] if token in chronicle.word_to_int else 0 for token in sequence ] \n","        for sequence in data ]\n","\n","        return (data, chronicle)\n","    \n","\n","class Filler(Chainer):\n","    \n","    def process(self, data, chronicle):\n","        ''' apply padding to numerical content '''\n","\n","        # assert numerical representation of input data\n","        assert(all(isinstance(token, int) for sequence in data for token in sequence))\n","        assert(hasattr(chronicle, 'sequence_max'))\n","\n","        # get the real size of each sequence\n","        chronicle.sizes = np.array([ len(sequence) - 1 for sequence in data ], dtype = \"longlong\")\n","\n","        # transform by padding\n","        data = np.array([ seq + [0] * (chronicle.sequence_max - len(seq)) for seq in data ], dtype = \"longlong\")\n","        \n","        return (data, chronicle)\n","\n","    \n","class Normalization(Chainer):\n","    \n","    def process(self, data, chronicle):\n","        \n","        # compute dispersion\n","        chronicle.std = np.std(data)\n","\n","        # compute mean\n","        chronicle.mean = np.mean(data)\n","        \n","        # normalize data\n","        data = np.array((data - chronicle.mean) / chronicle.std, dtype = \"float32\")\n","        \n","        return (data, chronicle)\n","\n","    def apply(self, data, chronicle):\n","        \n","        assert(chronicle.std is not None and chronicle.mean is not None)\n","        \n","        return self.process(data, chronicle)\n","\n","    def scale(self, data, chronicle):\n","        \n","        # undo the normalization\n","        data = np.array(data * chronicle.std + chronicle.mean, dtype = \"float32\")\n","        \n","        return (data, chronicle)\n","\n","    \n","class Composer(Chainer):\n","    \n","    def __init__(self, transforms):\n","        \n","        # current transformations\n","        self.transforms = transforms\n","        \n","        # check each transformation\n","        for transform in self.transforms:\n","            \n","            # check if it's chainer transformer\n","            if not isinstance(transform, Chainer):\n","                \n","                # list item is not an instance of Chainer transformer\n","                raise Exception(\"Illegal parameter, provide contiguous set of Chainer(s)\")\n","        \n","        # initialize chronicle of transformations\n","        self.chronicle = Chronicle()\n","    \n","    def process(self, raw):\n","        \n","        # initialize chainer data\n","        data = raw.copy()\n","        \n","        # apply transformations in series\n","        for transform in self.transforms:\n","            \n","            # process data\n","            data, _ = transform.process(data, self.chronicle)\n","                \n","        return (data, self.chronicle)\n","    \n","    def apply(self, raw):\n","        \n","        # create an independent chronicle\n","        chronicle = self.chronicle.copy()\n","        \n","        # initialize chainer data\n","        data = raw.copy()\n","        \n","        # apply transformations in series\n","        for transform in self.transforms:\n","            \n","            # apply transform to data\n","            data, _ = transform.apply(data, chronicle)  \n","                \n","        return (data, chronicle)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Jt0dmYyZgMb","colab_type":"text"},"source":["### Apply tranformations to features"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Az7vX2KHbtYa","colab":{}},"source":["# create composer for features\n","feature_composer = Composer([\n","    Tokenize(),\n","    Vocabulary(),\n","    TokenToNumeric(),\n","    Filler()\n","])\n","\n","# apply transformations and learn the vocabulary of our train dataset\n","features_train, features_train_chronicle = feature_composer.process(train_dataset[1])\n","\n","# apply transformations given existing learned vocabulary\n","features_valid, features_valid_chronicle = feature_composer.apply(valid_dataset[1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8UV685IAZgMf","colab_type":"text"},"source":["### Apply tranformations to targets"]},{"cell_type":"code","metadata":{"id":"Vh2wx0cMZgMf","colab_type":"code","colab":{}},"source":["# create composer for targets\n","target_composer = Composer([\n","    Normalization()\n","])\n","\n","# apply transformations and learn the characteristics \n","targets_train, targets_train_chronicle = target_composer.process(train_dataset[2].values)\n","\n","# apply transformations given existing learned characteristics\n","targets_valid, _ = target_composer.apply(valid_dataset[2].values)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YXvPGZSubtYe","colab":{}},"source":["# create the tensor datasets\n","train_tensor_dataset = TensorDataset(torch.from_numpy(features_train), torch.from_numpy(targets_train), torch.from_numpy(features_train_chronicle.sizes))\n","valid_tensor_dataset = TensorDataset(torch.from_numpy(features_valid), torch.from_numpy(targets_valid), torch.from_numpy(features_valid_chronicle.sizes))\n","\n","# create data loaders\n","train_loader = DataLoader(train_tensor_dataset, batch_size = batch_size, shuffle = True)\n","valid_loader = DataLoader(valid_tensor_dataset, batch_size = batch_size, shuffle = True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"b48nBt_QbtYf"},"source":["### Embedding Layer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Jho_unKcbtYf","colab":{}},"source":["class Model(nn.Module):\n","    \n","    def __init__(self, num_embeddings, embedding_dim, hidden_size, num_layers, output_size, bidirectional = False):\n","        super(Model, self).__init__()\n","        \n","        self.num_embeddings = num_embeddings\n","        self.embedding_dim = embedding_dim\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.output_size = output_size\n","        self.bidirectional = bidirectional\n","        \n","        # sparse(embedding) layer\n","        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n","        \n","        # recurrent neural network layer(lstm)\n","        self.rnn = nn.LSTM(self.embedding_dim, self.hidden_size, self.num_layers, \n","                               bidirectional = self.bidirectional, batch_first = True)\n","        \n","        # fully connected layer(linear) + dropout\n","        self.sec1 = nn.Sequential(\n","            nn.Linear(self.hidden_size, 128),\n","            nn.Dropout(0.15)\n","        )\n","        \n","        # fully connected layer(linear) + dropout\n","        self.sec2 = nn.Sequential(\n","            nn.Linear(128, self.output_size),\n","            nn.Dropout(0.15)\n","        )\n","    \n","    def forward(self, x, indices):\n","\n","        # embed words into dense representation\n","        x = self.embedding(x)\n","        \n","        # recurrent neural network; pass forward\n","        x, _ = self.rnn(x)\n","        \n","        # stack rnn output; relocate\n","        x = x.contiguous()\n","        \n","        # fully connected layer; pass forward while casually dropping cells\n","        x = self.sec1(x)\n","        \n","        # fully connected layer; pass forward while casually dropping cells\n","        x = self.sec2(x)\n","        \n","        # flatten the tensor to dimen 2\n","        x = x.view(batch_size, -1)\n","        \n","        # extract last prediction from the sequence\n","        x = x[torch.arange(len(indices)), indices]\n","        \n","        return x\n","    \n","    def save_model(self, checkpoint):\n","        ''' save model to dictionary '''\n","        \n","        assert(isinstance(checkpoint, dict))\n","        \n","        checkpoint[\"num_embeddings\"] = self.num_embeddings\n","        checkpoint[\"embedding_dim\"] = self.embedding_dim\n","        checkpoint[\"hidden_size\"] = self.hidden_size\n","        checkpoint[\"num_layers\"] = self.num_layers\n","        checkpoint[\"output_size\"] = self.output_size\n","        checkpoint[\"bidirectional\"] = self.bidirectional\n","        \n","        # save model internal weights\n","        checkpoint[\"state_dict\"] = self.state_dict()\n","        \n","        return checkpoint\n","        \n","    @staticmethod\n","    def load_model(checkpoint):\n","        ''' load model from dictionary '''\n","        \n","        assert(isinstance(checkpoint, dict))\n","        \n","        # create an instance of Model\n","        model = Model(\n","            checkpoint[\"num_embeddings\"],\n","            checkpoint[\"embedding_dim\"],\n","            checkpoint[\"hidden_size\"],\n","            checkpoint[\"num_layers\"],\n","            checkpoint[\"output_size\"],\n","            checkpoint[\"bidirectional\"]\n","        )\n","        \n","        # load weights\n","        model.load_state_dict(checkpoint[\"state_dict\"])\n","        \n","        return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"COfB7vE6btYh"},"source":["### Defining hyperparameters"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OLLYl69GbtYh","colab":{}},"source":["class Context():\n","    \n","    def __init__(self, model, learning_rate, verbose = True):\n","        \n","        assert(isinstance(model, Model))\n","        \n","        # device to be used\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        \n","        if verbose:\n","            \n","            print(f\"Model is moved to {self.device}\")\n","        \n","        # model to be trained\n","        self.model = model.to(self.device)\n","        \n","        # learning rate\n","        self.learning_rate = learning_rate\n","\n","        # loss function (mean squared error loss)\n","        self.criterion = nn.MSELoss(reduction = 'mean')\n","\n","        # optimizer with momentum\n","        self.optimizer = optim.Adam(self.model.parameters(), lr = self.learning_rate)\n","        \n","        # current validation min\n","        self.valid_loss_min = np.inf\n","    \n","    def create_scheduler(self):\n","        \n","        # custom scheduler\n","        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones = [4, 9], gamma = 0.72)\n","        \n","    def save_context(self, name):\n","        \n","        # current checkpoint to be saved\n","        checkpoint = {\n","            \"state_optim\" : self.optimizer.state_dict(),\n","            \"learning_rate\" : self.learning_rate,\n","            \"valid_loss_min\" : self.valid_loss_min\n","        }\n","        \n","        # save model params to dict\n","        checkpoint = self.model.save_model(checkpoint)\n","        \n","        # save checkpoint to disk\n","        torch.save(checkpoint, f\"{model_path}/{name}\")\n","        \n","    @staticmethod\n","    def load_context(name, context = None):\n","        \n","        # load latest checkpoint\n","        checkpoint = torch.load(f\"{model_path}/{name}\")\n","        \n","        # model loading\n","        model = Model.load_model(checkpoint)\n","        \n","        # recreate context\n","        context = Context(model, checkpoint[\"learning_rate\"])\n","        \n","        # update current validation loss\n","        context.valid_loss_min = checkpoint[\"valid_loss_min\"]\n","        \n","        # optimizer loading e.g state momentum\n","        context.optimizer.load_state_dict(checkpoint[\"state_optim\"])\n","        \n","        return context"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1558811673666,"user_tz":-120,"elapsed":8757,"user":{"displayName":"Cristian Chivriga","photoUrl":"","userId":"12583023746426163942"}},"id":"qWg1wX8xbtYj","outputId":"f7993b8f-cc11-43b8-e7aa-6bc377bf6631","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# our model\n","model = Model(features_train_chronicle.vocabulary_size, 512, 256, 2, 1)\n","\n","# context container for model characteristics\n","context = Context(model, learning_rate = 0.0001)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Model is moved to cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Q7U4AD_WbtYm"},"source":["### Model testing"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1558811673668,"user_tz":-120,"elapsed":2831,"user":{"displayName":"Cristian Chivriga","photoUrl":"","userId":"12583023746426163942"}},"id":"a29U7udYbtYm","outputId":"f00f6395-2dc6-4cd3-ae9b-148240b2cc63","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# singleton batch sample\n","feature_sample, target_sample, indices = next(iter(train_loader))\n","\n","# move to corresponding available\n","feature_sample, target_sample = feature_sample.to(context.device), target_sample.to(context.device)\n","\n","# forward pass\n","output = model.forward(feature_sample, indices)\n","\n","# compute lossop\n","loss = context.criterion(output, target_sample)\n","\n","print(f\"Loss is {loss.item()}\")"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Loss is 0.897085964679718\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zn0Cu-cibtYp","colab":{}},"source":["def valid_model(context, epoch, train_acc_loss):\n","    ''' model validation '''\n","    \n","    # the model to be validated\n","    model = context.model\n","        \n","    # validation loss\n","    valid_acc_loss = 0\n","    \n","    # evaluation mode\n","    model.eval()\n","\n","    with torch.no_grad():\n","\n","        for features, targets, indices in valid_loader:\n","            \n","            # move to corresponding available device\n","            features, targets = features.to(context.device), targets.to(context.device)\n","\n","            # forward pass\n","            output = model.forward(features, indices)\n","\n","            # calculate loss\n","            loss = context.criterion(output, targets)\n","            \n","            # update validation acc loss\n","            valid_acc_loss += loss.item() \n","\n","    # average loss \n","    valid_acc_loss = valid_acc_loss / len(valid_loader)\n","\n","    # print training / validation statistics \n","    print('Current Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","        epoch + 1, train_acc_loss, valid_acc_loss))\n","\n","    if(valid_acc_loss <= context.valid_loss_min):\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","            context.valid_loss_min, valid_acc_loss))\n","\n","        # update current best validation loss\n","        context.valid_loss_min = valid_acc_loss\n","\n","        # save best model\n","        context.save_context('model.pt')\n","        \n","\n","def train_model(context, epochs = 5, show_every_step = len(train_loader)):\n","    ''' model training '''\n","    \n","    # the model to be trained\n","    model = context.model\n","    \n","    # train loss and step counter\n","    train_acc_loss, step_counter = 0, 0\n","    \n","    # training mode\n","    model.train()\n","    \n","    for epoch in range(epochs):\n","        \n","        for features, targets, indices in train_loader:\n","            \n","            # removing accumulated gradients\n","            context.optimizer.zero_grad()\n","            \n","            # move to corresponding available device\n","            features, targets = features.to(context.device), targets.to(context.device)\n","            \n","            # forward pass\n","            output = model.forward(features, indices)\n","\n","            # calculate loss\n","            loss = context.criterion(output, targets)\n","            \n","            # calculate gradients\n","            loss.backward()\n","            \n","            # adjusting weights\n","            context.optimizer.step()\n","            \n","            # update loss acc\n","            train_acc_loss += loss.item()\n","            \n","            # update step counter\n","            step_counter += 1\n","            \n","            if(step_counter % show_every_step == 0):\n","                \n","                # average loss \n","                train_acc_loss = train_acc_loss / show_every_step\n","\n","                # print training / validation statistics \n","                valid_model(context, epoch, train_acc_loss)\n","                \n","                # reset train and step accumulator\n","                train_acc_loss, step_counter = 0, 0\n","                \n","                # training mode\n","                model.train()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0g14C8AYbtYq","colab":{"base_uri":"https://localhost:8080/","height":686},"outputId":"2aaddbc3-da77-4a72-a74b-6e4c128fc265","executionInfo":{"status":"ok","timestamp":1558813390228,"user_tz":-120,"elapsed":1352377,"user":{"displayName":"Cristian Chivriga","photoUrl":"","userId":"12583023746426163942"}}},"source":["# training model\n","train_model(context, 8, show_every_step = 2000)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Current Epoch: 1 \tTraining Loss: 0.426804 \tValidation Loss: 0.339788\n","Validation loss decreased (0.378453 --> 0.339788).  Saving model ...\n","Current Epoch: 1 \tTraining Loss: 0.423702 \tValidation Loss: 0.333102\n","Validation loss decreased (0.339788 --> 0.333102).  Saving model ...\n","Current Epoch: 1 \tTraining Loss: 0.407590 \tValidation Loss: 0.330842\n","Validation loss decreased (0.333102 --> 0.330842).  Saving model ...\n","Current Epoch: 2 \tTraining Loss: 0.400197 \tValidation Loss: 0.345221\n","Current Epoch: 2 \tTraining Loss: 0.372646 \tValidation Loss: 0.314250\n","Validation loss decreased (0.330842 --> 0.314250).  Saving model ...\n","Current Epoch: 2 \tTraining Loss: 0.371759 \tValidation Loss: 0.301952\n","Validation loss decreased (0.314250 --> 0.301952).  Saving model ...\n","Current Epoch: 2 \tTraining Loss: 0.366730 \tValidation Loss: 0.306131\n","Current Epoch: 3 \tTraining Loss: 0.345008 \tValidation Loss: 0.327538\n","Current Epoch: 3 \tTraining Loss: 0.344156 \tValidation Loss: 0.299589\n","Validation loss decreased (0.301952 --> 0.299589).  Saving model ...\n","Current Epoch: 3 \tTraining Loss: 0.337290 \tValidation Loss: 0.294478\n","Validation loss decreased (0.299589 --> 0.294478).  Saving model ...\n","Current Epoch: 4 \tTraining Loss: 0.328425 \tValidation Loss: 0.288173\n","Validation loss decreased (0.294478 --> 0.288173).  Saving model ...\n","Current Epoch: 4 \tTraining Loss: 0.305235 \tValidation Loss: 0.328207\n","Current Epoch: 4 \tTraining Loss: 0.322592 \tValidation Loss: 0.278366\n","Validation loss decreased (0.288173 --> 0.278366).  Saving model ...\n","Current Epoch: 4 \tTraining Loss: 0.314351 \tValidation Loss: 0.289826\n","Current Epoch: 5 \tTraining Loss: 0.291686 \tValidation Loss: 0.291312\n","Current Epoch: 5 \tTraining Loss: 0.287576 \tValidation Loss: 0.282716\n","Current Epoch: 5 \tTraining Loss: 0.291126 \tValidation Loss: 0.289401\n","Current Epoch: 6 \tTraining Loss: 0.286192 \tValidation Loss: 0.300556\n","Current Epoch: 6 \tTraining Loss: 0.263689 \tValidation Loss: 0.272083\n","Validation loss decreased (0.278366 --> 0.272083).  Saving model ...\n","Current Epoch: 6 \tTraining Loss: 0.266634 \tValidation Loss: 0.285238\n","Current Epoch: 6 \tTraining Loss: 0.273331 \tValidation Loss: 0.286379\n","Current Epoch: 7 \tTraining Loss: 0.251018 \tValidation Loss: 0.282969\n","Current Epoch: 7 \tTraining Loss: 0.249988 \tValidation Loss: 0.287017\n","Current Epoch: 7 \tTraining Loss: 0.247636 \tValidation Loss: 0.291808\n","Current Epoch: 8 \tTraining Loss: 0.246790 \tValidation Loss: 0.285908\n","Current Epoch: 8 \tTraining Loss: 0.227167 \tValidation Loss: 0.295845\n","Current Epoch: 8 \tTraining Loss: 0.227402 \tValidation Loss: 0.283826\n","Current Epoch: 8 \tTraining Loss: 0.237207 \tValidation Loss: 0.288502\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Lx1s9Au3btYu"},"source":["### Testing model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PjAUWQidbtYv","colab":{}},"source":["def pred_model(context):\n","    \n","    # evaluation mode\n","    context.model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        # valid features targets \n","        features, targets, indices = next(iter(train_loader))\n","\n","        # move to available device\n","        features, targets = features.to(context.device), targets.to(context.device)\n","\n","        # making prediction\n","        outputs = context.model.forward(features, indices).cpu()\n","\n","        # scale the outputs\n","        outputs, _ = target_composer.transforms[0].scale(outputs, targets_train_chronicle)\n","\n","        # scale the targets\n","        targets, _ = target_composer.transforms[0].scale(targets.cpu(), targets_train_chronicle)\n","\n","        for i in range(len(features)):\n","\n","            print(f\"Predicted score: {int(outputs[i])}, actual score {int(targets[i])}\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1558813702593,"user_tz":-120,"elapsed":592,"user":{"displayName":"Cristian Chivriga","photoUrl":"","userId":"12583023746426163942"}},"id":"SApcCDlVbtYx","outputId":"02989cda-cbd9-4c4f-bddb-d3fa0c3d8245","colab":{"base_uri":"https://localhost:8080/","height":298}},"source":["# making predictions\n","pred_model(context)"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Predicted score: 89, actual score 89\n","Predicted score: 88, actual score 88\n","Predicted score: 89, actual score 91\n","Predicted score: 91, actual score 92\n","Predicted score: 90, actual score 90\n","Predicted score: 89, actual score 90\n","Predicted score: 87, actual score 87\n","Predicted score: 83, actual score 83\n","Predicted score: 85, actual score 86\n","Predicted score: 87, actual score 89\n","Predicted score: 87, actual score 88\n","Predicted score: 87, actual score 86\n","Predicted score: 87, actual score 88\n","Predicted score: 88, actual score 88\n","Predicted score: 90, actual score 92\n","Predicted score: 87, actual score 87\n"],"name":"stdout"}]}]}