{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding for wine reviews and sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "    \n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import helper as hlp\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor as RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor as DecisionTreeRegressor\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "raw = pd.read_csv(\"./data/wines/wine_reviews.csv\", low_memory = False);\n",
    "\n",
    "# dropping unnecessary column\n",
    "raw = raw.drop(columns = [\"Unnamed: 0\"], inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(data, column, punctuation):\n",
    "    ''' utility function for text transformation for machine to interpret '''\n",
    "    \n",
    "    # make dataframe copy\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # transform each row to lowercase\n",
    "    data_copy[column] = data_copy[column].str.lower()\n",
    "    \n",
    "    # filter out punctuation\n",
    "    data_copy[column] = data_copy[column].str.replace('[^\\w\\s]', '')\n",
    "        \n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform each review to lowercase and remove punctuation\n",
    "raw_data = transform_text(raw, \"description\", punctuation)\n",
    "\n",
    "# transform non-numerical data to categorical\n",
    "hlp.trans_categorical(raw_data, labels = [\"description\"])\n",
    "\n",
    "# transform/normalize numeric data\n",
    "raw_numeric_data = hlp.transform_to_numeric(raw_data, suffle_data_frame = True)\n",
    "\n",
    "# split into features and targets\n",
    "features, targets = hlp.split_target(raw_numeric_data, \"points\")\n",
    "\n",
    "# training and validation data\n",
    "training_set, validation_set = hlp.split_data(features, targets, threshold = 1 / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(training_set[0][\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tdm = tfidf.fit_transform(training_set[0][\"description\"])\n",
    "\n",
    "valid_tdm = tfidf.transform(validation_set[0][\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.transform(validation_set[0][\"description\"].iloc[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(train_tdm, training_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(validation_set[1], lr.predict(valid_tdm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_verbose(description, model, transformer, expected_score = None):\n",
    "    \n",
    "    # generate tf-idf-weighted document-term matrix\n",
    "    description_tdm = transformer.transform([description])\n",
    "\n",
    "    # predict score given description\n",
    "    score = model.predict(description_tdm)[0]\n",
    "\n",
    "    print(f\"{description:100.100}...\\\n",
    "          \\n\\t output {score}, expected {expected_score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(data, model, transformer, count = 5):\n",
    "    \n",
    "    for index in range(count):\n",
    "        \n",
    "        # current description\n",
    "        description = data[0][\"description\"][index]\n",
    "        \n",
    "        # expected output\n",
    "        score = validation_set[1][index]\n",
    "        \n",
    "        validation_verbose(description, model, transformer, expected_score = score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some input from validation set\n",
    "validation(validation_set, lr, tfidf, count = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some custom input\n",
    "\n",
    "# average score\n",
    "validation_verbose(\"Pretty bad, can't handle the taste, extremely sour, how can someone make such wine?\", lr, tfidf)\n",
    "\n",
    "# good score\n",
    "validation_verbose(\"Amazing, fine vintage, delicious, rich texture that sobbing for more takes, just pure quality.\", lr, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Algorithms:\n",
    "    \n",
    "1. Embedding Layer\n",
    "2. Word2Vec\n",
    "    1. CBOW\n",
    "    2. C. Skip-Gram\n",
    "3. GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets; scores for each product and features\n",
    "features, targets = raw_data[\"description\"], raw_data[\"points\"]\n",
    "\n",
    "# normalize scores between [0, 1]\n",
    "targets = np.array(targets / 100, dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chainer(ABC):\n",
    "    ''' chainer class for chaining text transformations '''\n",
    "    \n",
    "    @abstractmethod\n",
    "    def process(self, data, chain = None):\n",
    "        ''' chain method for data preprocessing '''\n",
    "        pass\n",
    "    \n",
    "class Tokenize(Chainer):\n",
    "    \n",
    "    def process(self, data, chain):\n",
    "        ''' chain method for data preprocessing '''\n",
    "        \n",
    "        # tokenize, split a sentence by space\n",
    "        chain.data = data.str.split()\n",
    "        \n",
    "        # find maximum size of sequence of tokens\n",
    "        chain.sequence_max = max([ len(sequence) for sequence in chain.data ])\n",
    "        \n",
    "        return (chain.data, chain)\n",
    "    \n",
    "class Vocabulary(Chainer):\n",
    "    \n",
    "    def process(self, data, chain):\n",
    "        ''' create the known vocabulary basis '''\n",
    "        \n",
    "        # count token occurrences\n",
    "        chain.tokens = Counter([ token for sequence in data for token in sequence ])\n",
    "        \n",
    "        # vocabulary_size\n",
    "        chain.vocabulary_size = len(chain.tokens) + 1\n",
    "        \n",
    "        # word to integer mapping, 0 is reserved for padding\n",
    "        chain.word_to_int = { key : (index + 1) for index, key in enumerate(chain.tokens) }\n",
    "        \n",
    "        # integer to word mapping\n",
    "        chain.int_to_word = { index : word for word, index in chain.word_to_int.items() }\n",
    "        \n",
    "        return (data, chain)\n",
    "    \n",
    "class NumericToToken(Chainer):\n",
    "    \n",
    "    def process(self, data, chain):\n",
    "        ''' apply textual transformation '''\n",
    "        \n",
    "        assert(hasattr(chain, 'int_to_word'))\n",
    "        \n",
    "        # transform from textual to numerical representation\n",
    "        chain.data = [ [ chain.int_to_word[token] for token in sequence ] for sequence in data ]\n",
    "        \n",
    "        return (chain.data, chain)\n",
    "\n",
    "class TokenToNumeric(Chainer):\n",
    "    \n",
    "    def process(self, data, chain):\n",
    "        ''' apply numerical transformation '''\n",
    "\n",
    "        assert(hasattr(chain, 'word_to_int'))\n",
    "\n",
    "        # transform from textual to numerical representation\n",
    "        chain.data = [ [ chain.word_to_int[token] for token in sequence ] for sequence in data ]\n",
    "\n",
    "        return (chain.data, chain)\n",
    "           \n",
    "class Filler(Chainer):\n",
    "    \n",
    "    def process(self, data, chain):\n",
    "        ''' apply padding to numerical content '''\n",
    "\n",
    "        # assert numerical representation of input data\n",
    "        assert(all(isinstance(token, int) for sequence in data for token in sequence))\n",
    "        assert(hasattr(chain, 'sequence_max'))\n",
    "\n",
    "        # transform by padding\n",
    "        chain.data = [ sequence + [0] * (chain.sequence_max - len(sequence)) for sequence in data ]\n",
    "        \n",
    "        return (np.array(chain.data), chain)\n",
    "\n",
    "class Composer(Chainer):\n",
    "    \n",
    "    def __init__(self, transforms):\n",
    "        \n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def process(self, data, chain = None):\n",
    "        \n",
    "        # initialize chainer data\n",
    "        self.data = data\n",
    "        \n",
    "        # apply transformations in series\n",
    "        for transform in self.transforms:\n",
    "            \n",
    "            # check if it's chainer transformer\n",
    "            if(isinstance(transform, Chainer)):\n",
    "                \n",
    "                # update existing data, pass only current class reference\n",
    "                self.data, _ = transform.process(self.data, self)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # list item is not an instance of Chainer transformer\n",
    "                raise Exception(\"Illegal parameter, provide contiguous set of Chainer(s)\")\n",
    "                \n",
    "        return (self.data, self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transformations to our data\n",
    "features, chainer = Composer([\n",
    "    Tokenize(),\n",
    "    Vocabulary(),\n",
    "    TokenToNumeric(),\n",
    "    Filler()\n",
    "]).process(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation dataset\n",
    "train_dataset, valid_dataset = hlp.split_data(features, targets, threshold = 1 / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "batch_size = 16\n",
    "\n",
    "# create the tensor datasets\n",
    "train_tensor_dataset = TensorDataset(torch.from_numpy(train_dataset[0]).long(), torch.from_numpy(train_dataset[1]))\n",
    "valid_tensor_dataset = TensorDataset(torch.from_numpy(valid_dataset[0]).long(), torch.from_numpy(valid_dataset[1]))\n",
    "\n",
    "# create data loaders\n",
    "train_loader = DataLoader(train_tensor_dataset, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_tensor_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim, hidden_size, num_layers, output_size, bidirectional = False):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # sparse(embedding) layer\n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        \n",
    "        # recurrent neural network layer(lstm)\n",
    "        self.rnn = nn.LSTM(self.embedding_dim, self.hidden_size, self.num_layers, \n",
    "                               bidirectional = self.bidirectional, batch_first = True)\n",
    "        \n",
    "        # fully connected layer(linear) + dropout\n",
    "        self.sec1 = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, 128),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # fully connected layer(linear) + dropout\n",
    "        self.sec2 = nn.Sequential(\n",
    "            nn.Linear(128, self.output_size),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # embed words into dense representation\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # recurrent neural network; pass forward\n",
    "        x, _ = self.rnn(x)\n",
    "        \n",
    "        # stack rnn output\n",
    "        x = x.contiguous()\n",
    "        \n",
    "        # fully connected layer; pass forward while casually dropping cells\n",
    "        x = self.sec1(x)\n",
    "        \n",
    "        # fully connected layer; pass forward while casually dropping cells\n",
    "        x = self.sec2(x)\n",
    "        \n",
    "        # extract last prediction\n",
    "        x = x[:, -1].squeeze()\n",
    "        \n",
    "        print(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def save_model(self, checkpoint):\n",
    "        ''' save model to dictionary '''\n",
    "        \n",
    "        assert(isinstance(checkpoint, dict))\n",
    "        \n",
    "        checkpoint[\"num_embeddings\"] = self.num_embeddings\n",
    "        checkpoint[\"embedding_dim\"] = self.embedding_dim\n",
    "        checkpoint[\"hidden_size\"] = self.hidden_size\n",
    "        checkpoint[\"num_layers\"] = self.num_layers\n",
    "        checkpoint[\"output_size\"] = self.output_size\n",
    "        checkpoint[\"bidirectional\"] = self.bidirectional\n",
    "        \n",
    "        return checkpoint\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_model(self, checkpoint):\n",
    "        ''' load model from dictionary '''\n",
    "        \n",
    "        assert(isinstance(checkpoint, dict))\n",
    "        \n",
    "        # create an instance of Model\n",
    "        model = Model(\n",
    "            checkpoint[\"num_embeddings\"],\n",
    "            checkpoint[\"embedding_dim\"],\n",
    "            checkpoint[\"hidden_size\"],\n",
    "            checkpoint[\"num_layers\"],\n",
    "            checkpoint[\"output_size\"],\n",
    "            checkpoint[\"bidirectional\"]\n",
    "        )\n",
    "        \n",
    "        # load weights\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context():\n",
    "    \n",
    "    # path for context persistence\n",
    "    model_path = \"./models/\"\n",
    "    \n",
    "    def __init__(self, model, learning_rate):\n",
    "        \n",
    "        assert(isinstance(model, Model))\n",
    "        \n",
    "        # device to be used\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # model to be trained\n",
    "        self.model = model.to(self.device)\n",
    "        \n",
    "        # learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # loss function (mean squared error loss)\n",
    "        self.criterion = nn.MSELoss(reduction = 'mean')\n",
    "\n",
    "        # optimizer with momentum\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "    \n",
    "    def create_scheduler(self):\n",
    "        \n",
    "        # custom scheduler\n",
    "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones = [4, 9], gamma = 0.72)\n",
    "        \n",
    "    def save_context(self, name):\n",
    "        \n",
    "        # current checkpoint to be saved\n",
    "        checkpoint = {\n",
    "            \"state_optim\" : self.state_dict(),\n",
    "            \"learning_rate\" : self.learning_rate\n",
    "        }\n",
    "        \n",
    "        # save model params to dict\n",
    "        checkpoint = self.model.save_model(checkpoint)\n",
    "        \n",
    "        # save checkpoint to disk\n",
    "        torch.save(checkpoint, model_path + name)\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_context(self, context = None):\n",
    "        \n",
    "        # load latest checkpoint\n",
    "        checkpoint = torch.load(model_path)\n",
    "        \n",
    "        # model loading\n",
    "        model = Model.load_model(checkpoint)\n",
    "        \n",
    "        # recreate context\n",
    "        context = Context(model, checkpoint[\"learning_rate\"])\n",
    "        \n",
    "        # optimizer loading e.g state momentum\n",
    "        context.optimizer.load_state_dict(checkpoint[\"state_optim\"])\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model\n",
    "model = Model(chainer.vocabulary_size, 512, 256, 2, 1)\n",
    "\n",
    "# context container for model characteristics\n",
    "context = Context(model, learning_rate = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# singleton batch sample\n",
    "feature_sample, target_sample = next(iter(train_loader))\n",
    "\n",
    "# move to corresponding available\n",
    "feature_sample, target_sample = feature_sample.to(context.device), target_sample.to(context.device)\n",
    "\n",
    "# forward pass\n",
    "output = model.forward(feature_sample)\n",
    "\n",
    "# compute loss\n",
    "loss = context.criterion(output, target_sample) / batch_size\n",
    "\n",
    "print(f\"Loss is {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(context, epochs = 1, show_every_step = len(train_loader)):\n",
    "    \n",
    "    # the model to be trained\n",
    "    model = context.model\n",
    "    \n",
    "    # train loss and step counter\n",
    "    train_acc_loss, step_counter = 0, 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # training mode\n",
    "        model.train()\n",
    "        \n",
    "        for index, (features, targets) in enumerate(train_loader):\n",
    "            \n",
    "            # removing accumulated gradients\n",
    "            context.optimizer.zero_grad()\n",
    "            \n",
    "            # move to corresponding available device\n",
    "            features, targets = features.to(context.device), targets.to(context.device)\n",
    "            \n",
    "            # forward pass\n",
    "            outputs = model.forward(features)\n",
    "            \n",
    "            # calculate loss\n",
    "            loss = context.criterion(outputs, targets)\n",
    "            \n",
    "            # normalize loss by batch size\n",
    "            loss /= batch_size\n",
    "            \n",
    "            # calculate gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # adjusting weights\n",
    "            context.optimizer.step()\n",
    "            \n",
    "            # compute loss acc\n",
    "            train_acc_loss += loss.item() / batch_size\n",
    "            \n",
    "            # update step counter\n",
    "            step_counter += 1\n",
    "            \n",
    "            if(step_counter % show_every_step == 0):\n",
    "                \n",
    "                # average loss \n",
    "                train_acc_loss = train_acc_loss / len(train_loader)\n",
    "\n",
    "                # print training / validation statistics \n",
    "                print('Current Epoch: {} \\t Training Loss: {:.6f}'.format(\n",
    "                    epoch + 1, train_acc_loss))\n",
    "                \n",
    "                # reset train and step accumulator\n",
    "                train_acc_loss, step_counter = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model\n",
    "train_model(context, show_every_step = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_model(context):\n",
    "    \n",
    "    # evaluation mode\n",
    "    context.model.eval()\n",
    "    \n",
    "    # valid features targets \n",
    "    features, targets = next(iter(train_loader))\n",
    "    \n",
    "    # making prediction\n",
    "    outputs = context.model.forward(features)\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        \n",
    "        print(f\"Predicted score: {outputs[i] * 100}, actual score {targets[i] * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions\n",
    "pred_model(context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
