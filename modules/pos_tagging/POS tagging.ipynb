{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\DomainFlag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\DomainFlag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(ratio = 0.9):\n",
    "    assert (ratio >= 0 and ratio <= 1.0)\n",
    "    \n",
    "    # treebank corpus\n",
    "    file_names = nltk.corpus.treebank.fileids()\n",
    "    \n",
    "    # extract tagged words\n",
    "    tagged_words = []\n",
    "    for file_name in file_names:\n",
    "        tagged_words += list(nltk.corpus.treebank.tagged_words(file_name))\n",
    "        \n",
    "    # data offset\n",
    "    offset = int(len(tagged_words) * ratio)\n",
    "        \n",
    "    return (tagged_words[:offset], tagged_words[offset:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\" HMM model based on Search Beam & Viterbi algorithm \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \n",
    "        self.train_data = data\n",
    "        \n",
    "        # initialize the model\n",
    "        self.init(data)\n",
    "    \n",
    "    def init(self, data):\n",
    "                \n",
    "        # unpack word tokens and pos labels\n",
    "        tokens, labels = zip(*data)\n",
    "        \n",
    "        # our vocabulary\n",
    "        self.vocabulary, self.labels = set(tokens), set(labels)\n",
    "        self.label_types = list(self.labels)\n",
    "        \n",
    "        # freq token <-> pos label\n",
    "        self.freq_tokens = nltk.FreqDist(data)\n",
    "        \n",
    "        # create 2-gram generator\n",
    "        grams = nltk.bigrams(labels)\n",
    "\n",
    "        # create 1-gram freq and 2-gram freq for labels\n",
    "        self.freq_gram_labels = (nltk.FreqDist(labels), nltk.FreqDist(grams))\n",
    "    \n",
    "    def get_seq_prob(self, word, prev_target, target):\n",
    "        \n",
    "        # case if the target word is unknown\n",
    "        word_likelihood = 1.0 if word not in self.vocabulary else self.freq_tokens[(word, target)]\n",
    "            \n",
    "        return self.freq_gram_labels[1][(prev_target, target)] * word_likelihood / self.freq_gram_labels[0][prev_target] ** 2\n",
    "        \n",
    "    def forward(self, features, window_size = None):\n",
    "        \n",
    "        # avoid prob underflow\n",
    "        if window_size is None:\n",
    "            window_size = len(features)\n",
    "        \n",
    "        # sequence size\n",
    "        seq_size, label_size = len(features), len(self.label_types)\n",
    "        \n",
    "        # memoization of max prob for each t timestamp and indexing for backtracking\n",
    "        T1, T2 = np.zeros((window_size, label_size)), np.zeros((window_size, label_size))\n",
    "        \n",
    "        # initialize with equal prob\n",
    "        T1[0] = [ 1 / label_size ] * label_size\n",
    "        \n",
    "        # initialize helper variables\n",
    "        labels, offset = [], 0\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            # case of overflow\n",
    "            offset_lim = min(offset + window_size, seq_size) - offset\n",
    "            \n",
    "            # forward pass\n",
    "            for t in range(1, offset_lim):\n",
    "                \n",
    "                # current target token\n",
    "                token = features[t + offset]\n",
    "\n",
    "                # markov assumption\n",
    "                for index, label in enumerate(self.label_types):\n",
    "                    for prev_index, prev_label in enumerate(self.label_types):\n",
    "                        prob = model.get_seq_prob(token, prev_label, label) * T1[t - 1][prev_index]\n",
    "\n",
    "                        if prob > T1[t][index]:\n",
    "                            T1[t][index] = prob\n",
    "                            T2[t][index] = prev_index\n",
    "\n",
    "            # backward pass\n",
    "            prob_max_index = np.argmax(T1[-1, :])\n",
    "            \n",
    "            labels_window, prob_index = deque(), prob_max_index\n",
    "            for t in range(offset_lim, 1, -1):\n",
    "                prob_index = int(T2[t - 1][prob_index])\n",
    "                \n",
    "                labels_window.appendleft(self.label_types[prob_index])\n",
    "            \n",
    "            # reset mem\n",
    "            T1.fill(0)\n",
    "            T2.fill(0)\n",
    "            \n",
    "            # assign max prob to last word\n",
    "            T1[0][prob_max_index] = 1.0\n",
    "            \n",
    "            offset += window_size - 1\n",
    "            labels += list(labels_window)\n",
    "            \n",
    "            if offset >= seq_size:\n",
    "                break\n",
    "        \n",
    "        # update with last item\n",
    "        labels.append(self.label_types[prob_max_index])\n",
    "            \n",
    "        return labels           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of our model\n",
    "model = Model(train_data)\n",
    "\n",
    "# pre-process labelled data \n",
    "features, targets = zip(*valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.forward(features, window_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(outputs, targets):\n",
    "    assert (len(outputs) == len(targets))\n",
    "    \n",
    "    # compute number of valid labels\n",
    "    labels_predicted = 0\n",
    "    for predicted, target in zip(outputs, targets):\n",
    "        labels_predicted += predicted == target\n",
    "    \n",
    "    # size of annotations\n",
    "    size = len(targets)\n",
    "    \n",
    "    return labels_predicted / size, (labels_predicted, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model(output, targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
