{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vectors directly from the file\n",
    "word_embeddings = KeyedVectors.load_word2vec_format('./data/news_word_embeddings.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank', quiet = True)\n",
    "nltk.download('tagsets', quiet = True)\n",
    "nltk.download('stopwords', quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(ratio = 0.9):\n",
    "    assert (ratio >= 0 and ratio <= 1.0)\n",
    "    \n",
    "    # treebank corpus\n",
    "    file_names = nltk.corpus.treebank.fileids()\n",
    "    \n",
    "    # extract tagged words\n",
    "    tagged_words = []\n",
    "    for file_name in file_names:\n",
    "        tagged_words += list(nltk.corpus.treebank.tagged_words(file_name))\n",
    "        \n",
    "    # data offset\n",
    "    offset = int(len(tagged_words) * ratio)\n",
    "        \n",
    "    return (tagged_words[:offset], tagged_words[offset:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base:\n",
    "    \"\"\" Base model class \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    \"\"\" HMM model based on Search Beam & Viterbi algorithm \"\"\"\n",
    "    \n",
    "    max_suffix_size: int = 6\n",
    "    max_prefix_size: int = 4\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \n",
    "        self.train_data = data\n",
    "        \n",
    "        # initialize the model\n",
    "        self.init(data)\n",
    "    \n",
    "    def init(self, data):\n",
    "                \n",
    "        # unpack word tokens and pos labels\n",
    "        tokens, labels = zip(*data)\n",
    "        \n",
    "        # our vocabulary\n",
    "        self.vocabulary, self.labels = set(tokens), set(labels)\n",
    "        self.label_types = list(self.labels)\n",
    "        \n",
    "        # freq token <-> pos label\n",
    "        self.freq_dist = nltk.FreqDist(data)\n",
    "        self.freq_tokens = {}\n",
    "        \n",
    "        # resolve uppercase case\n",
    "        for (token, tag), freq in self.freq_dist.items():\n",
    "            self.freq_tokens[(token, tag)] = freq\n",
    "            if token.islower():\n",
    "                key = (token.capitalize(), tag)\n",
    "            else:\n",
    "                key = (token.lower(), tag)\n",
    "                \n",
    "            if key not in self.freq_tokens:\n",
    "                self.freq_tokens[key] = freq\n",
    "                \n",
    "        # generate token bounds\n",
    "        self.token_bounds = self.get_token_bounds()\n",
    "        \n",
    "        # create 2-gram generator\n",
    "        grams = nltk.bigrams(labels)\n",
    "\n",
    "        # create 1-gram freq and 2-gram freq for labels\n",
    "        self.freq_gram_labels = (nltk.FreqDist(labels), nltk.FreqDist(grams))\n",
    "        \n",
    "    def get_token_bounds(self, freq_trim = 10):\n",
    "        \n",
    "        # filter high-freq words (articles, prepositions)\n",
    "        freq_word_tags = { k: v for k, v in self.freq_tokens.items() if v < freq_trim }\n",
    "        \n",
    "        token_prefixes, token_suffixes = {}, {}\n",
    "        for (token, tag), v in self.freq_tokens.items():\n",
    "            size, index = len(token), self.label_types.index(tag)\n",
    "            \n",
    "            # counting prefixes\n",
    "            prefix_end = min(size - 1, self.max_prefix_size)\n",
    "            for i in range(1, prefix_end):\n",
    "                prefix = token[:i]\n",
    "                \n",
    "                if prefix not in token_prefixes:\n",
    "                    token_prefixes[prefix] = {\n",
    "                        'freq': np.zeros((len(self.label_types),)),\n",
    "                        'total': 0\n",
    "                    }\n",
    "                    \n",
    "                token_prefixes[prefix]['freq'][index] += v\n",
    "                token_prefixes[prefix]['total'] += v\n",
    "            \n",
    "            # counting suffixes\n",
    "            suffix_start = max(1, size - 1 - self.max_suffix_size)\n",
    "            for i in range(size - 1, suffix_start, -1):\n",
    "                suffix = token[i:]\n",
    "                \n",
    "                if suffix not in token_suffixes:\n",
    "                    token_suffixes[suffix] = {\n",
    "                        'freq': np.zeros((len(self.label_types),)),\n",
    "                        'total': 0\n",
    "                    }\n",
    "                    \n",
    "                token_suffixes[suffix]['freq'][index] += v\n",
    "                token_suffixes[suffix]['total'] += v\n",
    "                \n",
    "        return (token_suffixes, token_prefixes)\n",
    "    \n",
    "    def get_bound_prob(self, token, tag):\n",
    "        \n",
    "        prefixes_prob, suffixes_prob = [], []\n",
    "        size, index = len(token), self.label_types.index(tag)\n",
    "        \n",
    "        # compute prob suffix\n",
    "        suffix_start = max(1, size - 1 - self.max_suffix_size)\n",
    "        for i in range(size - 1, suffix_start, -1):\n",
    "            suffix = token[i:]\n",
    "            \n",
    "            if suffix in self.token_bounds[0]:\n",
    "                freq_bound = self.token_bounds[0][suffix]['freq'][index]\n",
    "                if suffix in self.token_bounds[0] and freq_bound != 0:\n",
    "                    suffixes_prob.append(freq_bound / self.token_bounds[0][suffix]['total'])\n",
    "                \n",
    "        # compute prob prefix\n",
    "        prefix_end = min(size - 1, self.max_prefix_size)\n",
    "        for i in range(1, prefix_end):\n",
    "            prefix = token[:i]\n",
    "            \n",
    "            if prefix in self.token_bounds[1]:\n",
    "                freq_bound = self.token_bounds[1][prefix]['freq'][index]\n",
    "                if prefix in self.token_bounds[1] and freq_bound != 0:\n",
    "                    prefixes_prob.append(freq_bound / self.token_bounds[1][prefix]['total'])\n",
    "                \n",
    "        # prefixes\n",
    "        weighted_left_prob = np.dot(prefixes_prob, prefixes_prob)\n",
    "        \n",
    "        # sufixes\n",
    "        weighted_right_prob = np.dot(suffixes_prob, suffixes_prob)\n",
    "                \n",
    "        return weighted_left_prob * weighted_right_prob\n",
    "        \n",
    "    def get_seq_prob(self, word, prev_tag, tag):\n",
    "        \n",
    "        # case if the target word is unknown\n",
    "        word_likelihood = None if word not in self.vocabulary else self.freq_tokens.get((word, tag), 0)\n",
    "        \n",
    "        if word_likelihood is None:\n",
    "            word_likelihood = self.get_bound_prob(word, tag)\n",
    "            \n",
    "        emission = word_likelihood / self.freq_gram_labels[0][tag]\n",
    "        transition = self.freq_gram_labels[1][(prev_tag, tag)] / self.freq_gram_labels[0][prev_tag]\n",
    "        \n",
    "        return emission * transition\n",
    "        \n",
    "    def forward(self, features, window_size = None):\n",
    "        \n",
    "        # avoid prob underflow\n",
    "        if window_size is None:\n",
    "            window_size = len(features)\n",
    "        \n",
    "        # sequence size\n",
    "        seq_size, label_size = len(features), len(self.label_types)\n",
    "        \n",
    "        # memoization of max prob for each t timestamp and indexing for backtracking\n",
    "        T1, T2 = np.zeros((window_size, label_size)), np.zeros((window_size, label_size))\n",
    "        \n",
    "        # initialize starting tag with maximum likelihood\n",
    "        token = features[0]\n",
    "        indices = [ self.freq_tokens.get((token, tag), 0) for tag in self.label_types ]\n",
    "        T1[0][int(np.argmax(indices))] = 1.0\n",
    "        \n",
    "        # initialize helper variables\n",
    "        labels, offset = [], 0\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            # case of overflow\n",
    "            offset_lim = min(offset + window_size, seq_size) - offset\n",
    "            \n",
    "            # forward pass\n",
    "            for t in range(1, offset_lim):\n",
    "                \n",
    "                # current target token\n",
    "                token = features[t + offset]\n",
    "\n",
    "                # markov assumption\n",
    "                for index, label in enumerate(self.label_types):\n",
    "                    for prev_index, prev_label in enumerate(self.label_types):\n",
    "                        prob = self.get_seq_prob(token, prev_label, label) * T1[t - 1][prev_index]\n",
    "\n",
    "                        if prob > T1[t][index]:\n",
    "                            T1[t][index] = prob\n",
    "                            T2[t][index] = prev_index\n",
    "\n",
    "            # backward pass\n",
    "            prob_max_index = np.argmax(T1[offset_lim - 1, :])\n",
    "            \n",
    "            labels_window, prob_index = deque(), prob_max_index\n",
    "            for t in range(offset_lim, 1, -1):\n",
    "                prob_index = int(T2[t - 1][prob_index])\n",
    "                \n",
    "                labels_window.appendleft(self.label_types[prob_index])\n",
    "            \n",
    "            # reset mem\n",
    "            T1.fill(0)\n",
    "            T2.fill(0)\n",
    "            \n",
    "            # assign max prob to last word\n",
    "            T1[0][prob_max_index] = 1.0\n",
    "            \n",
    "            offset += window_size - 1\n",
    "            labels += list(labels_window)\n",
    "            \n",
    "            if offset >= seq_size:\n",
    "                break\n",
    "        \n",
    "        # update with last item\n",
    "        labels.append(self.label_types[prob_max_index])\n",
    "            \n",
    "        return labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEMM:\n",
    "    \"\"\" MEMM model based on Search Beam & Viterbi algorithm \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \n",
    "        self.train_data = data\n",
    "        \n",
    "        # initialize the model\n",
    "        self.init(data)\n",
    "    \n",
    "    def init(self, data):\n",
    "                \n",
    "        # unpack word tokens and pos labels\n",
    "        tokens, labels = zip(*data)\n",
    "        \n",
    "        # our vocabulary\n",
    "        self.vocabulary, self.labels = set(tokens), set(labels)\n",
    "        self.label_types = list(self.labels)\n",
    "        \n",
    "        # create train features\n",
    "        inputs = []\n",
    "        for index, (token, tag) in enumerate(zip(tokens, labels)):\n",
    "            inputs.append(self.get_feature(tokens, labels, index))\n",
    "\n",
    "        # create train targets\n",
    "        targets = [ self.label_types.index(label) for label in labels ]\n",
    "        \n",
    "        # freq token <-> pos label\n",
    "        self.model = LogisticRegression(random_state = 0, max_iter = 300, solver = 'lbfgs', multi_class = 'multinomial').fit(inputs, targets)\n",
    "    \n",
    "    def get_feature(self, tokens, labels, index, window_size = 3, bare = False):\n",
    "        \n",
    "        # features\n",
    "        features = []\n",
    "        \n",
    "        # boundary indices\n",
    "        index_start, index_end = index - window_size, index + window_size\n",
    "\n",
    "        for i in range(index_start, index_end):\n",
    "\n",
    "            # current token\n",
    "            t = tokens[i] if i >= 0 and i < len(tokens) else None\n",
    "            \n",
    "            # get token vector representation\n",
    "            token_vec = word_embeddings[t] if t is not None and t in word_embeddings else np.zeros((word_embeddings.vector_size,))\n",
    "            \n",
    "            features.append(token_vec)\n",
    "            \n",
    "        if bare:\n",
    "            index_start, index = 0, len(labels)\n",
    "        \n",
    "        for i in range(index_start, index - 1):\n",
    "            \n",
    "            # current tag\n",
    "            t = labels[i] if i >= 0 and i < len(labels) else None\n",
    "            \n",
    "            if t == -1:\n",
    "                features.append([-1])\n",
    "            else:\n",
    "                features.append([self.label_types.index(t) if t is not None else -1])\n",
    "\n",
    "        # concatenate features\n",
    "        return np.concatenate(features)\n",
    "        \n",
    "    def forward(self, features, window_size = None):\n",
    "        \n",
    "        # avoid prob underflow\n",
    "        if window_size is None:\n",
    "            window_size = len(features)\n",
    "        \n",
    "        # sequence size\n",
    "        seq_size, label_size = len(features), len(self.label_types)\n",
    "        \n",
    "        # memoization of max prob for each t timestamp and indexing for backtracking\n",
    "        T1, T2 = np.zeros((window_size, label_size)), np.zeros((window_size, label_size))\n",
    "        \n",
    "        # initialize with equal prob\n",
    "        T1[0] = [ 1 / label_size ] * label_size\n",
    "        \n",
    "        # initialize helper variables\n",
    "        labels, offset = [], 0\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            # case of overflow\n",
    "            offset_lim = min(offset + window_size, seq_size) - offset\n",
    "            \n",
    "            # forward pass\n",
    "            for t in range(1, offset_lim):\n",
    "                \n",
    "                for prev_index, prev_tag in enumerate(self.label_types):\n",
    "                    \n",
    "                    # min_start index\n",
    "                    min_start, min_offset = max(0, t - 3), 0 if t - 3 >= 0 else t - 3\n",
    "\n",
    "                    # previous taggings based on maximum likelihood\n",
    "                    taggings, sample_index = deque(), int(prev_index)\n",
    "                    \n",
    "                    taggings.appendleft(self.label_types[sample_index])\n",
    "                    for i in range(t - 1, min_start, -1):\n",
    "                        sample_index = int(T2[i][sample_index])\n",
    "                        \n",
    "                        taggings.appendleft(self.label_types[sample_index])\n",
    "                        \n",
    "                    if min_offset < 0:\n",
    "                        taggings = labels[min_offset:] + list(taggings)\n",
    "                        \n",
    "                    if len(taggings) < 3:\n",
    "                        taggings = [-1] * (3 - len(taggings)) + taggings\n",
    "                    \n",
    "                    # features\n",
    "                    X = np.expand_dims(self.get_feature(features, taggings, t + offset, bare = True), 0)\n",
    "                    \n",
    "                    # make prediction (len(self.label_types), )\n",
    "                    prob = self.model.predict_proba(X)[0]\n",
    "                    \n",
    "                    # joint probability with previous assumptions\n",
    "                    prob *= T1[t - 1][prev_index]\n",
    "                    \n",
    "                    for index in range(len(self.label_types)):\n",
    "                        if prob[index] > T1[t][index]:\n",
    "                            T1[t][index] = prob[index]\n",
    "                            T2[t][index] = prev_index\n",
    "\n",
    "            # backward pass\n",
    "            prob_max_index = np.argmax(T1[-1, :])\n",
    "            \n",
    "            labels_window, prob_index = deque(), prob_max_index\n",
    "            for t in range(offset_lim, 1, -1):\n",
    "                prob_index = int(T2[t - 1][prob_index])\n",
    "                \n",
    "                labels_window.appendleft(self.label_types[prob_index])\n",
    "            \n",
    "            # reset mem\n",
    "            T1.fill(0)\n",
    "            T2.fill(0)\n",
    "            \n",
    "            # assign max prob to last word\n",
    "            T1[0][prob_max_index] = 1.0\n",
    "            \n",
    "            offset += window_size - 1\n",
    "            \n",
    "            # concatenate our outputs\n",
    "            labels += list(labels_window)\n",
    "            \n",
    "            if offset >= seq_size:\n",
    "                break\n",
    "        \n",
    "        # update with last item\n",
    "        labels.append(self.label_types[prob_max_index])\n",
    "            \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(network, valid_data, window_size = 7):\n",
    "\n",
    "    # pre-process labelled data \n",
    "    features, targets = zip(*valid_data)\n",
    "    \n",
    "    # generate network outputs\n",
    "    outputs = network.forward(features, window_size)\n",
    "    \n",
    "    # compute number of valid labels\n",
    "    labels_predicted = 0\n",
    "    for predicted, target in zip(outputs, targets):\n",
    "        labels_predicted += predicted == target\n",
    "    \n",
    "    # building the confusion matrix\n",
    "    mat = confusion_matrix(targets, outputs, labels = network.label_types)\n",
    "    \n",
    "    # size of annotations\n",
    "    size = len(targets)\n",
    "    \n",
    "    return labels_predicted / size, (labels_predicted, size), mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_type, train_data, valid_data, n = 6):\n",
    "    \n",
    "    # reference model definition\n",
    "    model = HMM if model_type == 'hmm' else MEMM\n",
    "    network = model(train_data)\n",
    "    \n",
    "    # validate the model\n",
    "    score, occ, mat = validate_model(network, valid_data)\n",
    "    print(f\"Model {model_type} -> {score}\")\n",
    "    \n",
    "    # labels size (tags)\n",
    "    size = len(network.label_types)\n",
    "    \n",
    "    # remove positive true\n",
    "    mat = ~(np.identity(size) == 1) * mat\n",
    "    \n",
    "    # building the data frame with corresponding labels and extracting the n largest NT\n",
    "    mat = pd.DataFrame(mat, index = network.label_types, columns = network.label_types)\n",
    "    mat_largest_n = mat.unstack().nlargest(n)\n",
    "    \n",
    "    display(mat_largest_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hmm -> 0.9229241160111243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NN   NNP    85\n",
       "NNP  NN     73\n",
       "NN   JJ     39\n",
       "VBN  VBD    31\n",
       "NN   VBG    27\n",
       "JJ   NN     26\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DomainFlag\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memm -> 0.849523241954708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       ",   IN        70\n",
       "    CC        48\n",
       "CC  IN        39\n",
       "    ,         30\n",
       ",   .         28\n",
       "    -NONE-    28\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_type in ['hmm', 'memm']:\n",
    "    test_model(model_type, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of our model\n",
    "network = HMM(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = [\n",
    "    'The weather is pretty much ok for sightseeing',\n",
    "    'I said her that her lines are well written',\n",
    "    'Tea is much better than coffe',\n",
    "    'Like it or not, we did a good job'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather is pretty much ok for sightseeing -> ['RP', 'RP', 'RP', 'RP', 'RP', 'RP', 'RP', 'RP']\n",
      "I said her that her lines are well written -> ['PRP', 'VBD', 'PRP', 'IN', 'PRP$', 'NNS', 'VBP', 'RB', 'VBN']\n",
      "Tea is much better than coffe -> ['NN', 'VBZ', 'RB', 'JJR', 'IN', 'NNP']\n",
      "Like it or not, we did a good job -> ['RP', 'RP', 'RP', 'RP', 'RP', 'RP', 'RP', 'RP', 'RP']\n"
     ]
    }
   ],
   "source": [
    "for s in seqs:\n",
    "    output = network.forward(s.split(' '))\n",
    "    \n",
    "    print(f\"{s} -> {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
