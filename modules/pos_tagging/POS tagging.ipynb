{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vectors directly from the file\n",
    "word_embeddings = KeyedVectors.load_word2vec_format('./data/news_word_embeddings.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\DomainFlag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\DomainFlag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(ratio = 0.9):\n",
    "    assert (ratio >= 0 and ratio <= 1.0)\n",
    "    \n",
    "    # treebank corpus\n",
    "    file_names = nltk.corpus.treebank.fileids()\n",
    "    \n",
    "    # extract tagged words\n",
    "    tagged_words = []\n",
    "    for file_name in file_names:\n",
    "        tagged_words += list(nltk.corpus.treebank.tagged_words(file_name))\n",
    "        \n",
    "    # data offset\n",
    "    offset = int(len(tagged_words) * ratio)\n",
    "        \n",
    "    return (tagged_words[:offset], tagged_words[offset:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base:\n",
    "    \"\"\" Base model class \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    \"\"\" HMM model based on Search Beam & Viterbi algorithm \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \n",
    "        self.train_data = data\n",
    "        \n",
    "        # initialize the model\n",
    "        self.init(data)\n",
    "    \n",
    "    def init(self, data):\n",
    "                \n",
    "        # unpack word tokens and pos labels\n",
    "        tokens, labels = zip(*data)\n",
    "        \n",
    "        # our vocabulary\n",
    "        self.vocabulary, self.labels = set(tokens), set(labels)\n",
    "        self.label_types = list(self.labels)\n",
    "        \n",
    "        # freq token <-> pos label\n",
    "        self.freq_tokens = nltk.FreqDist(data)\n",
    "        \n",
    "        # create 2-gram generator\n",
    "        grams = nltk.bigrams(labels)\n",
    "\n",
    "        # create 1-gram freq and 2-gram freq for labels\n",
    "        self.freq_gram_labels = (nltk.FreqDist(labels), nltk.FreqDist(grams))\n",
    "    \n",
    "    def get_seq_prob(self, word, prev_target, target):\n",
    "        \n",
    "        # case if the target word is unknown\n",
    "        word_likelihood = 1.0 if word not in self.vocabulary else self.freq_tokens[(word, target)]\n",
    "            \n",
    "        return self.freq_gram_labels[1][(prev_target, target)] * word_likelihood / self.freq_gram_labels[0][prev_target] ** 2\n",
    "        \n",
    "    def forward(self, features, window_size = None):\n",
    "        \n",
    "        # avoid prob underflow\n",
    "        if window_size is None:\n",
    "            window_size = len(features)\n",
    "        \n",
    "        # sequence size\n",
    "        seq_size, label_size = len(features), len(self.label_types)\n",
    "        \n",
    "        # memoization of max prob for each t timestamp and indexing for backtracking\n",
    "        T1, T2 = np.zeros((window_size, label_size)), np.zeros((window_size, label_size))\n",
    "        \n",
    "        # initialize with equal prob\n",
    "        T1[0] = [ 1 / label_size ] * label_size\n",
    "        \n",
    "        # initialize helper variables\n",
    "        labels, offset = [], 0\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            # case of overflow\n",
    "            offset_lim = min(offset + window_size, seq_size) - offset\n",
    "            \n",
    "            # forward pass\n",
    "            for t in range(1, offset_lim):\n",
    "                \n",
    "                # current target token\n",
    "                token = features[t + offset]\n",
    "\n",
    "                # markov assumption\n",
    "                for index, label in enumerate(self.label_types):\n",
    "                    for prev_index, prev_label in enumerate(self.label_types):\n",
    "                        prob = self.get_seq_prob(token, prev_label, label) * T1[t - 1][prev_index]\n",
    "\n",
    "                        if prob > T1[t][index]:\n",
    "                            T1[t][index] = prob\n",
    "                            T2[t][index] = prev_index\n",
    "\n",
    "            # backward pass\n",
    "            prob_max_index = np.argmax(T1[-1, :])\n",
    "            \n",
    "            labels_window, prob_index = deque(), prob_max_index\n",
    "            for t in range(offset_lim, 1, -1):\n",
    "                prob_index = int(T2[t - 1][prob_index])\n",
    "                \n",
    "                labels_window.appendleft(self.label_types[prob_index])\n",
    "            \n",
    "            # reset mem\n",
    "            T1.fill(0)\n",
    "            T2.fill(0)\n",
    "            \n",
    "            # assign max prob to last word\n",
    "            T1[0][prob_max_index] = 1.0\n",
    "            \n",
    "            offset += window_size - 1\n",
    "            labels += list(labels_window)\n",
    "            \n",
    "            if offset >= seq_size:\n",
    "                break\n",
    "        \n",
    "        # update with last item\n",
    "        labels.append(self.label_types[prob_max_index])\n",
    "            \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEMM:\n",
    "    \"\"\" MEMM model based on Search Beam & Viterbi algorithm \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \n",
    "        self.train_data = data\n",
    "        \n",
    "        # initialize the model\n",
    "        self.init(data)\n",
    "    \n",
    "    def init(self, data):\n",
    "                \n",
    "        # unpack word tokens and pos labels\n",
    "        tokens, labels = zip(*data)\n",
    "        \n",
    "        # our vocabulary\n",
    "        self.vocabulary, self.labels = set(tokens), set(labels)\n",
    "        self.label_types = list(self.labels)\n",
    "        \n",
    "        # create train features\n",
    "        inputs = []\n",
    "        for index, (token, tag) in enumerate(zip(tokens, labels)):\n",
    "            inputs.append(self.get_feature(tokens, labels, index))\n",
    "\n",
    "        # create train targets\n",
    "        targets = [ self.label_types.index(label) for label in labels ]\n",
    "        \n",
    "        # freq token <-> pos label\n",
    "        self.model = LogisticRegression(random_state = 0, max_iter = 300, solver = 'lbfgs', multi_class = 'multinomial').fit(inputs, targets)\n",
    "    \n",
    "    def get_feature(self, tokens, labels, index, window_size = 3, bare = False):\n",
    "        \n",
    "        # features\n",
    "        features = []\n",
    "        \n",
    "        # boundary indices\n",
    "        index_start, index_end = index - window_size, index + window_size\n",
    "\n",
    "        for i in range(index_start, index_end):\n",
    "\n",
    "            # current token\n",
    "            t = tokens[i] if i >= 0 and i < len(tokens) else None\n",
    "            \n",
    "            # get token vector representation\n",
    "            token_vec = word_embeddings[t] if t is not None and t in word_embeddings else np.zeros((word_embeddings.vector_size,))\n",
    "            \n",
    "            features.append(token_vec)\n",
    "            \n",
    "        if bare:\n",
    "            index_start, index = 0, len(labels)\n",
    "        \n",
    "        for i in range(index_start, index - 1):\n",
    "            \n",
    "            # current tag\n",
    "            t = labels[i] if i >= 0 and i < len(labels) else None\n",
    "            \n",
    "            if t == -1:\n",
    "                features.append([-1])\n",
    "            else:\n",
    "                features.append([self.label_types.index(t) if t is not None else -1])\n",
    "\n",
    "        # concatenate features\n",
    "        return np.concatenate(features)\n",
    "        \n",
    "    def forward(self, features, window_size = None):\n",
    "        \n",
    "        # avoid prob underflow\n",
    "        if window_size is None:\n",
    "            window_size = len(features)\n",
    "        \n",
    "        # sequence size\n",
    "        seq_size, label_size = len(features), len(self.label_types)\n",
    "        \n",
    "        # memoization of max prob for each t timestamp and indexing for backtracking\n",
    "        T1, T2 = np.zeros((window_size, label_size)), np.zeros((window_size, label_size))\n",
    "        \n",
    "        # initialize with equal prob\n",
    "        T1[0] = [ 1 / label_size ] * label_size\n",
    "        \n",
    "        # initialize helper variables\n",
    "        labels, offset = [], 0\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            # case of overflow\n",
    "            offset_lim = min(offset + window_size, seq_size) - offset\n",
    "            \n",
    "            # forward pass\n",
    "            for t in range(1, offset_lim):\n",
    "                \n",
    "                for prev_index, prev_tag in enumerate(self.label_types):\n",
    "                    \n",
    "                    # min_start index\n",
    "                    min_start, min_offset = max(0, t - 3), 0 if t - 3 >= 0 else t - 3\n",
    "\n",
    "                    # previous taggings based on maximum likelihood\n",
    "                    taggings, sample_index = deque(), int(prev_index)\n",
    "                    \n",
    "                    taggings.appendleft(self.label_types[sample_index])\n",
    "                    for i in range(t - 1, min_start, -1):\n",
    "                        sample_index = int(T2[i][sample_index])\n",
    "                        \n",
    "                        taggings.appendleft(self.label_types[sample_index])\n",
    "                        \n",
    "                    if min_offset < 0:\n",
    "                        taggings = labels[min_offset:] + list(taggings)\n",
    "                        \n",
    "                    if len(taggings) < 3:\n",
    "                        taggings = [-1] * (3 - len(taggings)) + taggings\n",
    "                    \n",
    "                    # features\n",
    "                    X = np.expand_dims(self.get_feature(features, taggings, t + offset, bare = True), 0)\n",
    "                    \n",
    "                    # make prediction (len(self.label_types), )\n",
    "                    prob = self.model.predict_proba(X)[0]\n",
    "                    \n",
    "                    # joint probability with previous assumptions\n",
    "                    prob *= T1[t - 1][prev_index]\n",
    "                    \n",
    "                    for index in range(len(self.label_types)):\n",
    "                        if prob[index] > T1[t][index]:\n",
    "                            T1[t][index] = prob[index]\n",
    "                            T2[t][index] = prev_index\n",
    "\n",
    "            # backward pass\n",
    "            prob_max_index = np.argmax(T1[-1, :])\n",
    "            \n",
    "            labels_window, prob_index = deque(), prob_max_index\n",
    "            for t in range(offset_lim, 1, -1):\n",
    "                prob_index = int(T2[t - 1][prob_index])\n",
    "                \n",
    "                labels_window.appendleft(self.label_types[prob_index])\n",
    "            \n",
    "            # reset mem\n",
    "            T1.fill(0)\n",
    "            T2.fill(0)\n",
    "            \n",
    "            # assign max prob to last word\n",
    "            T1[0][prob_max_index] = 1.0\n",
    "            \n",
    "            offset += window_size - 1\n",
    "            \n",
    "            # concatenate our outputs\n",
    "            labels += list(labels_window)\n",
    "            \n",
    "            if offset >= seq_size:\n",
    "                break\n",
    "        \n",
    "        # update with last item\n",
    "        labels.append(self.label_types[prob_max_index])\n",
    "            \n",
    "        return labels           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(outputs, targets):\n",
    "    assert (len(outputs) == len(targets))\n",
    "    \n",
    "    # compute number of valid labels\n",
    "    labels_predicted = 0\n",
    "    for predicted, target in zip(outputs, targets):\n",
    "        labels_predicted += predicted == target\n",
    "    \n",
    "    # size of annotations\n",
    "    size = len(targets)\n",
    "    \n",
    "    return labels_predicted / size, (labels_predicted, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_type, train_data, valid_data):\n",
    "    \n",
    "    # reference model definition\n",
    "    model = HMM if model_type == 'hmm' else MEMM\n",
    "    \n",
    "    # create an instance of our model\n",
    "    network = model(train_data)\n",
    "    \n",
    "    # pre-process labelled data \n",
    "    features, targets = zip(*valid_data)\n",
    "    \n",
    "    # generate network outputs\n",
    "    outputs = network.forward(features, window_size = 7)\n",
    "    \n",
    "    score, occ = validate_model(outputs, targets)\n",
    "\n",
    "    print(f\"Model {model_type} -> {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hmm -> 0.8915375446960667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DomainFlag\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memm -> 0.850715137067938\n"
     ]
    }
   ],
   "source": [
    "for model_type in ['hmm', 'memm']:\n",
    "    test_model(model_type, train_data, valid_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
