{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import helper as hlp\n",
    "\n",
    "from pandas.api.types import is_categorical_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "raw = pd.read_csv(\"./data/wine_reviews.csv\", low_memory = False);\n",
    "\n",
    "# dropping unnecessary column\n",
    "raw = raw.drop(columns = [\"Unnamed: 0\", \"description\"], inplace = False)\n",
    "\n",
    "raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform non-numerical data to categorical\n",
    "hlp.trans_categorical(raw, labels = [\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform/normalize numerical data\n",
    "features, targets = hlp.trans_numerical(raw, \"points\", suffle_data_frame = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller subset of training and validation data\n",
    "training_set, validation_set = hlp.split_data(features, targets, threshold = 1 / 8, subset = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "    \n",
    "    def __init__(self, criterion = \"mse\", min_samples_leaf = 3):\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        \n",
    "        # left/right nodes\n",
    "        self.nodes = None\n",
    "        \n",
    "        # best split score\n",
    "        self.split_score = np.inf\n",
    "        \n",
    "        # feature(column, value) to be split on \n",
    "        self.feature_column, self.feature_value = None, None\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "            \n",
    "        for column in x.columns:\n",
    "            \n",
    "            # find best split for a col\n",
    "            split(x, y, column)\n",
    "                    \n",
    "        if(self.split_score != np.inf):\n",
    "            \n",
    "            # splitting selection indices\n",
    "            split_indices = x[column] < value\n",
    "            \n",
    "            # splitting features and targets\n",
    "            split_dataset = [ x[split_indices], y[split_indices] ], [ x[~split_indices], y[~split_indices] ]\n",
    "            \n",
    "            # creating nodes\n",
    "            self.nodes = [ Node(self.criterion, self.min_samples_leaf) for i in range(0, 2) ]\n",
    "            \n",
    "            for index, node in enumerate(self.nodes):\n",
    "            \n",
    "                # recursively fit two models with smaller datasets\n",
    "                node.fit(* split_dataset[index])\n",
    "                \n",
    "    def std(self, sum_acc, sum_power_acc, count):\n",
    "        ''' compute std(x) = sqrt(E(x ^ 2) - E(x) ^ 2) '''\n",
    "        \n",
    "        return np.sqrt(sum_power_acc / count - (sum_acc / count) ** 2)\n",
    "        \n",
    "                \n",
    "    def split(self, x, y, column):\n",
    "        ''' split based on std(x) = sqrt(E(x ^ 2) - E(x) ^ 2) '''\n",
    "        \n",
    "        # reset indices\n",
    "        x = x.reset_index(drop = True)\n",
    "        \n",
    "        # sorted indices\n",
    "        sorting_indices = np.argsort(x[column])\n",
    "        \n",
    "        # sum of power to 2 accumulators\n",
    "        left_power_acc, right_power_acc = 0, np.sum([ target ** 2 for target in y ])\n",
    "        \n",
    "        # sum accumulators\n",
    "        left_acc, right_acc = 0, np.sum(y)\n",
    "        \n",
    "        for index, split_index in enumerate(sorting_indices):\n",
    "            \n",
    "            # updating left power of 2 sum\n",
    "            left_power_acc += y[split_index] ** 2\n",
    "            \n",
    "            # updating right power of 2 sum\n",
    "            right_power_acc -= y[split_index] ** 2\n",
    "            \n",
    "            \n",
    "            # update left sum acc\n",
    "            left_acc += y[split_index]\n",
    "            \n",
    "            # update right sum acc\n",
    "            right_acc -= y[split_index]\n",
    "            \n",
    "            \n",
    "            if(index < self.min_samples_leaf - 1 or index > y.size - self.min_samples_leaf - 1):\n",
    "                continue\n",
    "\n",
    "            # mean of standard deviation of 2 subsets\n",
    "            score = (self.std(left_acc, left_power_acc, index + 1) + \n",
    "                         self.std(right_acc, right_power_acc, y.size - (index + 1))) / 2\n",
    "\n",
    "            if(score < self.split_score):\n",
    "\n",
    "                # update current best score\n",
    "                self.split_score = score\n",
    "\n",
    "                # update current best column and value for the split \n",
    "                self.feature_column, self.feature_value = column, x[column][split_index]\n",
    "            \n",
    "\n",
    "class DecisionTree():\n",
    "    \n",
    "    def __init__(self, criterion = \"mse\", max_features = 0.6, min_samples_leaf = 4):\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.max_features = max_features\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        \n",
    "        # root decision node\n",
    "        self.root = None\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        \n",
    "        # initialize decision root node\n",
    "        self.root = DecisionNode(self.criterion, self.min_samples_leaf)\n",
    "        \n",
    "        # fit decision tree to our dataset\n",
    "        self.root.fit(x, y)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \n",
    "        if(self.root != None):\n",
    "            \n",
    "            # make actual prediction through full pass\n",
    "            return self.root.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestRegressor():\n",
    "    \n",
    "    def __init__(self, estimators = 30, criterion = \"mse\", max_features = 0.6, min_samples_leaf = 4, shuffle = True):\n",
    "        \n",
    "        self.estimators = estimators\n",
    "        self.criterion = criterion\n",
    "        self.max_features = max_features\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # instantiate trees\n",
    "        self.trees = [ DecisionTree() for i in range(0, estimators) ]\n",
    "        \n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        \n",
    "        # assert that both features and targets are same size\n",
    "        assert(x.shape[0] == y.size)\n",
    "        \n",
    "        # assert targets 1D vector of continous data\n",
    "        assert(y.ndim == 1)\n",
    "        \n",
    "        self.x, self.y = x, y\n",
    "            \n",
    "        if(self.shuffle):\n",
    "            \n",
    "            # create a random permutation of indices\n",
    "            self.indices = np.random.permutation(y.size)\n",
    "            \n",
    "            # shuffle our features and targets based on computed permutation\n",
    "            self.x, self.y = self.x.iloc(self.indices), self.y.iloc(self.indices)\n",
    "            \n",
    "        \n",
    "        for tree in self.trees:\n",
    "            \n",
    "            # fit every decision tree regresor to our dataset\n",
    "            tree.fit(self.x, self.y)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \n",
    "        # mean of tree predictors\n",
    "        return [ tree.predict(x) for tree in self.trees ] / self.estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = RandomForestRegressor(estimators = 15, criterion = \"mse\", max_features = 0.6, min_samples_leaf = 4, shuffle = False)\n",
    "\n",
    "node = DecisionNode()\n",
    "\n",
    "%timeit node.split(* training_set, \"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Split value: {node.split_score}, feature value: {node.feature_value} on {node.feature_column}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
